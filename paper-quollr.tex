% !TeX root = RJwrapper.tex
\title{quollr: An R Package for Visalizing 2D Models in High Dimensional Space}


\author{by Jayani P.G. Lakshika, Dianne Cook, Paul Harrison, Michael Lydeamore, and Thiyanga S. Talagala}

\maketitle

\abstract{%
An abstract of less than 150 words.
}

\begin{verbatim}
#library(quollr)
library(knitr)
library(kableExtra)
library(readr)
library(ggplot2)
library(dplyr)
library(ggbeeswarm)
library(Rtsne)
library(umap)
library(phateR)
library(reticulate)
library(rsample)

set.seed(20230531)

use_python("~/miniforge3/envs/pcamp_env/bin/python")
use_condaenv("pcamp_env")

reticulate::source_python(paste0(here::here(), "/scripts/function_scripts/Fit_PacMAP_code.py"))
reticulate::source_python(paste0(here::here(), "/scripts/function_scripts/Fit_TriMAP_code.py"))
\end{verbatim}

\begin{verbatim}
library(scales)
s_curve_noise_umap$UMAP1 <- rescale(s_curve_noise_umap$UMAP1)

y_min <- -sqrt(3)/2
y_max <- sqrt(3)/2

s_curve_noise_umap$UMAP2 <- ((s_curve_noise_umap$UMAP2 - min(s_curve_noise_umap$UMAP2))/(max(s_curve_noise_umap$UMAP2) - min(s_curve_noise_umap$UMAP2))) * (y_max - y_min)

# Define the desired size of the hexagons
hex_size <- 0.2

# Compute the horizontal spacing (dx) and vertical spacing (dy) between hexagon centers
dx <- sqrt(3) * hex_size
dy <- 1.5 * hex_size 

x_length <- 1
y_length_scaled <- diff(range(s_curve_noise_umap$UMAP2))

# Compute the number of bins along the x-axis and y-axis
# num_bins_x <- ceiling(x_length / dx)
# num_bins_y <- ceiling(y_length_scaled / dy)
\end{verbatim}

\begin{verbatim}
# s_curve_noise_umap[,1:(NCOL(s_curve_noise_umap) - 1)] <- scale(s_curve_noise_umap[,1:(NCOL(s_curve_noise_umap) - 1)])
\end{verbatim}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\hypertarget{usage}{%
\subsection{Usage}\label{usage}}

\begin{itemize}
\tightlist
\item
  dependencies
\end{itemize}

\begin{verbatim}
library(tools)
package_dependencies("quollr")
\end{verbatim}

\begin{itemize}
\tightlist
\item
  basic example
\end{itemize}

\hypertarget{datasets}{%
\subsubsection{Datasets}\label{datasets}}

The \texttt{quollr} package comes with several data sets that load with the package. These are described in Table \ref{tab:datasets-tb-pdf}.

\begin{table}

\caption{\label{tab:datasets-tb-pdf}quollr datasets}
\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{4cm}>{\raggedright\arraybackslash}p{8cm}}
\toprule
data & explanation\\
\midrule
s\_curve\_noise & Simulated 3D S-curve data with additional four noise dimensions.\\
s\_curve\_noise\_training & Training data derived from S-curve data.\\
s\_curve\_noise\_test & Test data derived from S-curve data.\\
s\_curve\_noise\_umap & UMAP 2D embedding data of S-curve data (n\_neighbors: 15, min\_dist: 0.1).\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{compute-hexagonal-bin-configurations}{%
\subsubsection{Compute hexagonal bin configurations}\label{compute-hexagonal-bin-configurations}}

Hexagonal binning is a powerful technique for visualizing the density of data points in a 2-d space. Unlike traditional rectangular bins, hexagonal bins offer several advantages, including a more uniform representation of density and reduced visual bias. However, to effectively do the hexagonal binning, it's essential to compute the appropriate configurations based on the characteristics of the dataset.

Before computing hexagonal bin configurations, we need to determine the range of data along the x and y axes to establish the boundary for hexagonal binning. Additionally, choosing an appropriate hexagon size (the radius of the outer circle) is essential. By default, the \texttt{calculate\_effective\_x\_bins()} and \texttt{calculate\_effective\_y\_bins()} functions use a hexagon size of 1.07457. However, users can adjust the hexagon size to fit the data range and achieve regular hexagons without overlapping.

The hexagon size directly affects the number of bins generated. A higher hexagonal size will result in fewer bins, while a lower hexagonal size will lead to more bins. Therefore, there's always a trade-off depending on the dataset used. Users should consider their specific data characteristics when selecting the hexagon size.

\begin{verbatim}
num_bins_x <- calculate_effective_x_bins(.data = s_curve_noise_umap,
                                         x = "UMAP1", hex_size = 0.2)
num_bins_x 
\end{verbatim}

\begin{verbatim}
#> [1] 4
\end{verbatim}

\begin{verbatim}
num_bins_y <- calculate_effective_y_bins(.data = s_curve_noise_umap,
                                         y = "UMAP2", hex_size = 0.2)
num_bins_y 
\end{verbatim}

\begin{verbatim}
#> [1] 7
\end{verbatim}

\hypertarget{generate-full-hex-grid}{%
\subsubsection{Generate full hex grid}\label{generate-full-hex-grid}}

Generating full hexagonal grid contains main three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate all the hexagonal bin centroids
\end{enumerate}

Steps:

\begin{itemize}
\item
  First compute hex grid bound values along the x and y axis and generate the all the points within the hex box
\item
  Second for each x-value, find which y values are in the even row
\item
  Then, shift the x values of the even rows
\end{itemize}

\begin{verbatim}
all_centroids_df <- generate_full_grid_centroids(nldr_df = s_curve_noise_umap, 
                                                 x = "UMAP1", y = "UMAP2", 
                                                 num_bins_x = num_bins_x, 
                                                 num_bins_y = num_bins_y, 
                                                 x_start = NA, y_start = NA, 
                                                 hex_size = 0.2)

glimpse(all_centroids_df)
\end{verbatim}

\begin{verbatim}
#> Rows: 28
#> Columns: 2
#> $ x <dbl> 0.0000000, 0.3464102, 0.6928203, 1.0392305, 0.1732051, 0.5196152, 0.~
#> $ y <dbl> 0.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.3, 0.3, 0.6, 0.6, 0.6, 0.6, 0.9, 0.9~
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Generate hexagonal coordinates
\end{enumerate}

Steps:
- Compute horizontal width of the hexagon

\begin{itemize}
\item
  Compute vertical width of the hexagon and multiply by a factor for overlapping (\(sqrt(3) / 2 * 1.15\))
\item
  Obtain hexagon polygon coordinates
\item
  Obtain the number of hexagons in the full grid
\item
  Generate the coordinates for the hexagons
\end{itemize}

\begin{verbatim}
hex_grid <- gen_hex_coordinates(all_centroids_df, hex_size = 0.2)
glimpse(hex_grid)
\end{verbatim}

\begin{verbatim}
#> Rows: 168
#> Columns: 6
#> $ c_x   <dbl> 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000~
#> $ c_y   <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,~
#> $ x     <dbl> 0.1732051, 0.1732051, 0.0000000, -0.1732051, -0.1732051, 0.00000~
#> $ y     <dbl> 0.09959292, -0.09959292, -0.19918584, -0.09959292, 0.09959292, 0~
#> $ id    <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4~
#> $ hexID <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4~
\end{verbatim}

\begin{verbatim}
ggplot(data = hex_grid, aes(x = x, y = y)) + geom_polygon(fill = "white", color = "black", aes(group = id)) +
  geom_point(aes(x = c_x, y = c_y), color = "red") +
  coord_fixed()
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{verbatim}
full_grid_centroids_with_hexbin_id <- hex_grid |>
  dplyr::select("c_x", "c_y", "hexID") |>
    dplyr::distinct() 

ggplot(data = hex_grid, aes(x = x, y = y)) + geom_polygon(fill = "white", color = "black", aes(group = id)) +
  geom_text(aes(x = c_x, y = c_y, label = hexID)) +
  coord_fixed()
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Assign data into hexagons
\end{enumerate}

\begin{itemize}
\item
  Compute distances between nldr coordinates and hex bin centroids
\item
  Find the hexagonal centroid that have the minimum distance
\end{itemize}

\begin{verbatim}
s_curve_noise_umap_with_id <- assign_data(s_curve_noise_umap, full_grid_centroids_with_hexbin_id)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Compute standardized counts
\end{enumerate}

\begin{itemize}
\item
  Compute number of data points within each hexagon
\item
  Compute standardise count by dividing the counts by the maximum
\end{itemize}

\begin{verbatim}
df_with_std_counts <- compute_std_counts(nldr_df = s_curve_noise_umap_with_id)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Extract full grid info
\end{enumerate}

\begin{itemize}
\item
  Assign standardize counts for hex bins
\item
  Join with the hexagonal coordinates
\end{itemize}

\begin{verbatim}
hex_full_count_df <- generate_full_grid_info(hex_grid, df_with_std_counts)
\end{verbatim}

\begin{verbatim}
ggplot(data = hex_grid, aes(x = x, y = y)) + geom_polygon(fill = "white", color = "black", aes(group = id)) +
  geom_point(data = s_curve_noise_umap, aes(x = UMAP1, y = UMAP2), color = "blue") +
  coord_fixed()
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-15-1.pdf}

\begin{verbatim}
ggplot(data = hex_full_count_df, aes(x = x, y = y)) +
  geom_polygon(color = "black", aes(group = id, fill = std_counts)) +
  geom_text(aes(x = c_x, y = c_y, label = hexID)) +
  scale_fill_viridis_c(direction = -1, na.value = "#ffffff") +
  coord_fixed()
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-16-1.pdf}

\hypertarget{buffer-size}{%
\paragraph{Buffer size}\label{buffer-size}}

When generating hexagonal bins in R, a buffer is often included to ensure that the data points are evenly distributed within the bins and to prevent edge effects. The buffer helps in two main ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Preventing Edge Effects}: Without a buffer, the outermost data points might fall near the boundary of the hexagonal grid, leading to incomplete bins or uneven distribution of data. By adding a buffer, you create a margin around the outer edges of the grid, ensuring that all data points are fully enclosed within the bins.
\item
  \textbf{Ensuring Even Distribution}: The buffer allows for a smoother transition between adjacent bins. This helps in cases where data points are not perfectly aligned with the grid lines, ensuring that each data point is assigned to the nearest bin without bias towards any specific direction.
\end{enumerate}

Overall, including a buffer when generating hexagonal bins helps to produce more accurate and robust binning results, particularly when dealing with real-world data that may have irregular distributions or boundary effects.

\hypertarget{construct-the-2d-model-with-different-options}{%
\subsubsection{Construct the 2D model with different options}\label{construct-the-2d-model-with-different-options}}

\begin{verbatim}
df_bin_centroids <- hex_full_count_df[complete.cases(hex_full_count_df[["std_counts"]]), ] |>
  dplyr::select("c_x", "c_y", "hexID", "std_counts") |>
  dplyr::distinct() |>
  dplyr::rename(c("x" = "c_x", "y" = "c_y"))
  
df_bin_centroids
\end{verbatim}

\begin{verbatim}
#>            x   y hexID std_counts
#> 1  0.0000000 0.0     1 0.40000000
#> 2  0.3464102 0.0     2 0.26666667
#> 3  0.1732051 0.3     5 0.86666667
#> 4  0.5196152 0.3     6 0.13333333
#> 5  0.3464102 0.6    10 0.20000000
#> 6  0.6928203 0.6    11 0.53333333
#> 7  0.8660254 0.9    15 0.80000000
#> 8  0.6928203 1.2    19 0.06666667
#> 9  0.5196152 1.5    22 0.06666667
#> 10 0.8660254 1.5    23 1.00000000
#> 11 0.6928203 1.8    27 0.46666667
#> 12 1.0392305 1.8    28 0.20000000
\end{verbatim}

\hypertarget{construct-the-high-d-model-with-different-options}{%
\subsubsection{Construct the high-D model with different options}\label{construct-the-high-d-model-with-different-options}}

\begin{verbatim}
## To generate a data set with high-D and 2D training data
df_all <- training_data |> dplyr::select(-ID) |>
  dplyr::bind_cols(s_curve_noise_umap_with_id)

## To generate averaged high-D data

df_bin <- avg_highD_data(.data = df_all, column_start_text = "x") ## Need to pass ID column name
\end{verbatim}

\hypertarget{generate-the-triangular-mesh}{%
\subsubsection{Generate the triangular mesh}\label{generate-the-triangular-mesh}}

\begin{verbatim}
tr1_object <- triangulate_bin_centroids(df_bin_centroids, x = "x", y = "y")
tr_from_to_df <- generate_edge_info(triangular_object = tr1_object)
\end{verbatim}

\hypertarget{compute-parameter-defaults}{%
\subsubsection{Compute parameter defaults}\label{compute-parameter-defaults}}

\hypertarget{shift-the-hexagonal-grid-origin}{%
\paragraph{Shift the hexagonal grid origin}\label{shift-the-hexagonal-grid-origin}}

If shift\_x happen to the positive direction of x it should input as a positive value, if not other way
If shift\_y happen to the positive direction of y it should input as a positive value, if not other way

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Assign shift along the x and y axis (limited the amount should less than the cell\_diameter)
\item
  Generate bounds with shift origin
\end{enumerate}

\hypertarget{benchmark-value-to-remove-the-low-density-hexagons}{%
\paragraph{Benchmark value to remove the low-density hexagons}\label{benchmark-value-to-remove-the-low-density-hexagons}}

\hypertarget{benchmark-value-to-remove-the-long-edges}{%
\paragraph{Benchmark value to remove the long edges}\label{benchmark-value-to-remove-the-long-edges}}

\begin{verbatim}
## Compute 2D distances
distance <- cal_2d_dist(tr_from_to_df_coord = tr_from_to_df)

## To plot the distribution of distance
plot_dist <- function(distance_df){
  distance_df$group <- "1"
  dist_plot <- ggplot(distance_df, aes(x = group, y = distance)) +
    geom_quasirandom()+
    ylim(0, max(unlist(distance_df$distance))+ 0.5) + coord_flip()
  return(dist_plot)
}

plot_dist(distance)
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-20-1.pdf}

\begin{verbatim}
benchmark <- find_benchmark_value(distance_edges = distance, distance_col = "distance")
benchmark
\end{verbatim}

\begin{verbatim}
#> [1] 0.6
\end{verbatim}

\begin{verbatim}
benchmark <- 0.75
\end{verbatim}

\hypertarget{model-function}{%
\subsubsection{Model function}\label{model-function}}

\texttt{fit\_high\_d\_model()} function is used to generate the 2D model and the high-D model.

\begin{verbatim}
## To generate a data set with high-D and 2D training data
df_all <- training_data |> dplyr::select(-ID) |>
  dplyr::bind_cols(s_curve_noise_umap_with_id)

## To generate averaged high-D data

df_bin <- avg_highD_data(.data = df_all, column_start_text = "x") ## Need to pass ID column name
\end{verbatim}

\hypertarget{predict-2d-embeddings}{%
\subsubsection{Predict 2D embeddings}\label{predict-2d-embeddings}}

To predict the 2D embeddings for a new data point using the trained high-D model, we follow a series of steps outlined below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Compute high-dimensional Euclidean distance}: Calculate the Euclidean distance between each new data point and the lift model's coordinates in the high-dimensional space. This distance metric helps identify the nearest lift model coordinates to each new data point.
\item
  \textbf{Find the nearest lift model coordinates}: Determine the lift model's high-dimensional coordinates that are closest to each new data point based on the computed distances. This step helps establish a correspondence between the new data points and the lift model's representation in the high-dimensional space.
\item
  \textbf{Map the hexagonal bin ID}: Assign each new data point to a hexagonal bin based on its nearest lift model coordinates. This mapping ensures that each data point is associated with a specific region in the hexagonal grid representation of the high-dimensional space.
\item
  \textbf{Map the hexagonal bin centroid coordinates in 2D}: Transform the hexagonal bin centroid coordinates from the high-dimensional space to the 2D embedding space using the trained lift model. This mapping provides the 2D embeddings for the new data points, allowing them to be visualized and analyzed in a lower-dimensional space.
\end{enumerate}

The \texttt{predict\_2d\_embeddings()} function is used to predict 2D embeddings for a new data point. The inputs for the function are test data, bin centroid coordinates in 2D, lifting model coordinates in high-D, and the type of the NLDR technique. The output contains the predicted 2D embeddings along with the predicted hexagonal id and the ID of the test data.

\begin{verbatim}
pred_df_test <- predict_2d_embeddings(test_data = training_data,
df_bin_centroids = df_bin_centroids, df_bin = df_bin, type_NLDR = "UMAP")

glimpse(pred_df_test)
\end{verbatim}

\begin{verbatim}
#> Rows: 75
#> Columns: 4
#> $ pred_UMAP_1 <dbl> 0.1732051, 0.8660254, 0.8660254, 0.0000000, 0.5196152, 0.6~
#> $ pred_UMAP_2 <dbl> 0.3, 0.9, 0.9, 0.0, 0.3, 1.8, 0.9, 0.6, 0.9, 1.8, 0.6, 0.3~
#> $ ID          <int> 1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 19, 20, 21~
#> $ pred_hb_id  <int> 5, 15, 15, 1, 6, 27, 15, 11, 15, 28, 11, 5, 23, 5, 11, 22,~
\end{verbatim}

\hypertarget{making-summaries}{%
\subsubsection{Making summaries}\label{making-summaries}}

There are two important summaries that should be made when constructing the model on a dataset using a specific NLDR technique, the Mean Square Error (MSE) and Akaike Information Criterion (AIC) which measure prediction accuracy. The function that perform the summaries called \texttt{generate\_eval\_df()}. The output of this function is a list which contains MSE and AIC. The code below uses \texttt{generate\_eval\_df()} to find the MSE and the AIC values for the model constructed on UMAP 2D embeddings generated from the \texttt{s\_curve\_noise} training data.

\begin{verbatim}
generate_summary(test_data = training_data, prediction_df = pred_df_test,
                 df_bin = df_bin, col_start = "x")
\end{verbatim}

\begin{verbatim}
#> $mse
#> [1] 0.2612147
#> 
#> $aic
#> [1] -536.7666
\end{verbatim}

\hypertarget{visualizations}{%
\subsubsection{Visualizations}\label{visualizations}}

We use static visualizations to understand the model constructed in 2D. On the other hand, dynamic visualization is used to see how the model looks in high-D space.

\hypertarget{static-visualizations}{%
\paragraph{Static visualizations}\label{static-visualizations}}

Static visualizations main involves two types of results. One is the triangulation and the other is the long edge removal. Both types of visualizations provide ggplot objects.

\begin{itemize}
\item
  Triangulation result: To visualize the results of triangulation, we input a dataset containing hexagonal bin centroid coordinates where 2D embedding data exists. \texttt{geom\_trimesh()} is used to visualize this result.
\item
  Long edge removal: The long edge removal process involves identifying and removing long edges from the triangular mesh. Table shows the main arguments of the functions. We offer two functions for visualizing this process:
\item
  \texttt{colour\_long\_edges()}: This function colors the long edges within the triangular mesh by red.
\item
  \texttt{remove\_long\_edges()}: After identifying long edges, this function draws the triangular mesh without the long edges.
\end{itemize}

\hypertarget{dynamic-visaulizations}{%
\paragraph{Dynamic visaulizations}\label{dynamic-visaulizations}}

The \texttt{show\_langevitour()} function enables dynamic visualization of the 2D model alongside the high-dimensional (high-D) data in its original space. This visualization is facilitated by langevitour object, allowing users to interactively explore the relationship between the 2D embeddings and the underlying high-dimensional data. The main arguments are shown in Table

\begin{verbatim}
trimesh <- ggplot(df_bin_centroids, aes(x = x, y = y)) +
  geom_point(size = 0.1) +
  geom_trimesh() +
  coord_equal()

trimesh
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-24-1.pdf}

\begin{verbatim}
trimesh_gr <- colour_long_edges(distance_edges = distance, benchmark_value = benchmark,
                                tr_from_to_df_coord = tr_from_to_df, distance_col = "distance")

trimesh_gr
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-25-1.pdf}

\begin{verbatim}
trimesh_removed <- remove_long_edges(distance_edges = distance, benchmark_value = benchmark,
                                     tr_from_to_df_coord = tr_from_to_df, distance_col = "distance")
trimesh_removed
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-26-1.pdf}

\begin{verbatim}
tour1 <- show_langevitour(df_all, df_bin, df_bin_centroids, 
                          benchmark_value = benchmark,
                          distance = distance, distance_col = "distance", 
                          use_default_benchmark_val = FALSE, 
                          column_start_text = "x")
tour1
\end{verbatim}

\includegraphics{paper-quollr_files/figure-latex/unnamed-chunk-27-1.pdf}

\hypertarget{find-non-empty-bins}{%
\subsubsection{Find non-empty bins}\label{find-non-empty-bins}}

\hypertarget{tests}{%
\subsection{Tests}\label{tests}}

All functions have tests written and implemented using the \CRANpkg{testthat} (Wickham 2011) in R.

\hypertarget{application}{%
\section{Application}\label{application}}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

This article is created using \CRANpkg{knitr} (Xie 2015) and \CRANpkg{rmarkdown} (Xie, Allaire, and Grolemund 2018) in R with the \texttt{rjtools::rjournal\_article} template. The source code for reproducing this paper can be found at: \url{https://github.com/JayaniLakshika/paper-quollr}.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-testthat}{}}%
Wickham, Hadley. 2011. {``Testthat: Get Started with Testing.''} \emph{The R Journal} 3: 5--10. \url{https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-knitr}{}}%
Xie, Yihui. 2015. \emph{Dynamic Documents with {R} and Knitr}. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. \url{https://yihui.name/knitr/}.

\leavevmode\vadjust pre{\hypertarget{ref-rmarkdown}{}}%
Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. \emph{{R} Markdown: The Definitive Guide}. Boca Raton, Florida: Chapman; Hall/CRC. \url{https://bookdown.org/yihui/rmarkdown}.

\end{CSLReferences}


\address{%
Jayani P.G. Lakshika\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
\url{https://jayanilakshika.netlify.app/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-6265-6481}{0000-0002-6265-6481}}\\%
\email{jayani.piyadigamage@monash.edu}%
}

\address{%
Dianne Cook\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
\url{http://www.dicook.org/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3813-7155}{0000-0002-3813-7155}}\\%
\href{mailto:dicook@monash.edu}{\nolinkurl{dicook@monash.edu}}%
}

\address{%
Paul Harrison\\
Monash University\\%
MGBP, BDInstitute, VIC 3800 Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3980-268X}{0000-0002-3980-268X}}\\%
\href{mailto:paul.harrison@monash.edu}{\nolinkurl{paul.harrison@monash.edu}}%
}

\address{%
Michael Lydeamore\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0001-6515-827X}{0000-0001-6515-827X}}\\%
\href{mailto:michael.lydeamore@monash.edu}{\nolinkurl{michael.lydeamore@monash.edu}}%
}

\address{%
Thiyanga S. Talagala\\
University of Sri Jayewardenepura\\%
Department of Statistics, Gangodawila, Nugegoda 10100 Sri Lanka\\
%
\url{https://thiyanga.netlify.app/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-0656-9789}{0000-0002-0656-9789}}\\%
\href{mailto:ttalagala@sjp.ac.lk}{\nolinkurl{ttalagala@sjp.ac.lk}}%
}
