% !TeX root = RJwrapper.tex
\title{quollr: An R Package for Visualizing 2-D Models from Non-linear Dimension Reductions in High Dimensional Space}


\author{by Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, and Thiyanga S. Talagala}

\maketitle

\abstract{%
Non-linear dimension reduction (NLDR) methods provide a low-dimensional representation of high-dimensional data (\(p\text{-}D\)) by applying a non-linear transformation. However, the complexity of the transformations and data structures can create wildly different representations depending on the method and (hyper)parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures. The R package \CRANpkg{quollr} has been developed as a new visual tool to determine which method and which (hyper)parameter choices provide the most accurate representation of high-dimensional data. The \texttt{scurve} data from the package is used to illustrate the algorithm. Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles are used to demonstrate the usability of the package.
}

\section{Introduction}\label{introduction}

Non-linear dimension reduction (NLDR) techniques, such as t-distributed stochastic neighbor embedding (tSNE) \citep{laurens2008}, uniform manifold approximation and projection (UMAP) \citep{leland2018}, potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm \citep{moon2019}, large-scale dimensionality reduction Using triplets (TriMAP) \citep{amid2019}, and pairwise controlled manifold approximation (PaCMAP) \citep{yingfan2021}, create wildly different representations depending on the selected method and (hyper)parameter choices (Figure \ref{fig:nldr-layouts}). It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures.

This paper presents the R package, \texttt{quollr} which introduce a new visual tool to evaluate which NLDR method and which (hyper)parameter choice gives the most accurate representation of high-dimensional data. The methodology of this algorithm is explained in \emph{cite the methodology paper}. The software is available from the Comprehensive R Archive Network (CARN) at \url{https://CRAN.R-project.org/package=quollr}.

\begin{figure}
\includegraphics[width=1\linewidth,alt={NLDR representations with different methods and (hyper)parameter choices.}]{paper-quollr_files/figure-latex/nldr-layouts-1} \caption{Six different NLDR representations of Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles have been generated using different techniques and different (hyper)parameter choices. Researchers may have seen any of these in their analysis of this data, depending on their choice of method, or typical (hyper)parameter choice. Would their downstream analysis decisions vary based on the version they observe? Which representation most accurately represents the structure in high-dimensional space?}\label{fig:nldr-layouts}
\end{figure}

The paper is organized as follows. The next section introduces the implementation of the \texttt{quollr} package on CRAN, including a demonstration of the package's key functions and visualization capabilities. In the application section, we illustrate the algorithm's functionality for studying clustering data structures. Finally, we conclude the paper with a brief summary and discuss potential opportunities for using our algorithm.

\section{Implementation}\label{implementation}

\subsection{Installation}\label{installation}

The package can be installed from CRAN:

\begin{verbatim}
install.packages("quollr")
\end{verbatim}

and the development version can be installed from GitHub:

\begin{verbatim}
devtools::install_github("JayaniLakshika/quollr")
\end{verbatim}

\subsection{Web site}\label{web-site}

More documentation of the package can be found at the web site \url{https://jayanilakshika.github.io/quollr/}.

\subsection{Package dependencies}\label{package-dependencies}

Understanding the dependencies of the \texttt{quollr} package is essential for smooth operation and error prevention. The following dependencies refer to the other R packages that \texttt{quollr} relies on to execute its functions effectively.

\begin{verbatim}
#> $quollr
#>  [1] "dplyr"       "ggplot2"     "grid"        "interp"      "langevitour"
#>  [6] "proxy"       "rlang"       "rsample"     "stats"       "tibble"     
#> [11] "tidyselect"
\end{verbatim}

\subsection{Data sets}\label{data-sets}

The \texttt{quollr} package comes with several data sets that load with the package. These are described in Table \ref{tab:datasets-tb-pdf}.

\begin{table}

\caption{\label{tab:datasets-tb-pdf}quollr data sets}
\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{4cm}>{\raggedright\arraybackslash}p{8cm}}
\toprule
data & explanation\\
\midrule
scurve & Simulated data to generate a S-curve with additional noise dimensions.\\
scurve\_umap & The UMAP embedding with n\_neighbors \$46\$ and min\_dist \$0.9\$ for `S-curve`.\\
scurve\_umap\_predict & The predicted UMAP embedding with n\_neighbors \$46\$ and min\_dist \$0.9\$ for `S-curve`.\\
scurve\_umap2 & The UMAP embedding with n\_neighbors \$10\$ and min\_dist \$0.4\$ for `S-curve`.\\
scurve\_umap3 & The UMAP embedding with n\_neighbors \$62\$ and min\_dist \$0.1\$ for `S-curve`.\\
\addlinespace
scurve\_umap4 & The UMAP embedding with n\_neighbors \$30\$ and min\_dist \$0.5\$ for `S-curve`.\\
scurve\_umap5 & The UMAP embedding with n\_neighbors \$15\$ and min\_dist \$0.5\$ for `S-curve`.\\
scurve\_umap6 & The UMAP embedding with n\_neighbors \$15\$ and min\_dist \$0.1\$ for `S-curve`.\\
scurve\_model\_obj & The object contains the scaled NLDR object (`nldr\_obj`), the hexagonal object (`hb\_obj`), the fitted model in both \$2\textbackslash{}text\{-\}D\$ (`model\_2d`), and \$p\textbackslash{}text\{-\}D\$ (`model\_highd`), and triangular mesh (`trimesh\_data`).\\
scurve\_umap\_mse & The MSE corresponding to different binwidths for the model generated using `scurve\_umap`.\\
\addlinespace
scurve\_umap\_mse2 & The MSE corresponding to different binwidths for the model generated using `scurve\_umap2`.\\
scurve\_umap\_mse3 & The MSE corresponding to different binwidths for the model generated using `scurve\_umap3`.\\
scurve\_umap\_mse4 & The MSE corresponding to different binwidths for the model generated using `scurve\_umap4`.\\
scurve\_umap\_mse5 & The MSE corresponding to different binwidths for the model generated using `scurve\_umap5`.\\
scurve\_umap\_mse6 & The MSE corresponding to different binwidths for the model generated using `scurve\_umap6`.\\
\bottomrule
\end{tabular}
\end{table}

The following demonstration of the package's functionality assumes \texttt{quollr} has been loaded. We also want to load the built-in data sets \texttt{scurve} and \texttt{scurve\_umap}.

\texttt{scurve} is a \(7\text{-}D\) simulated dataset. It is constructed by simulating \(5000\) observations from \(\theta \sim U(-3\pi/2, 3\pi/2)\), \(X_1 = \sin(\theta)\), \(X_2 \sim U(0, 2)\) (adding thickness to the S), \(X_3 = \text{sign}(\theta) \times (\cos(\theta) - 1)\). The remaining variables \(X_4, X_5, X_6, X_7\) are all uniform error, with small variance. \texttt{scurve\_umap} is the UMAP \(2\text{-}D\) embedding for \texttt{scurve} data with \texttt{n\_neighbors} is \(46\) and \texttt{min\_dist} is \(0.9\). Each data set contains a unique ID column that maps \texttt{scurve} and \texttt{scurve\_umap}.

\subsection{Main function}\label{main-function}

The mains steps for the algorithm can be executed by the main function \texttt{fit\_highd\_model()}, or can be run separately for more flexibility.

This function requires several parameters: the high-dimensional data (\texttt{highd\_data}), the emdedding data (\texttt{nldr\_data}), the number of bins along the x-axis (\texttt{bin1}), the buffer amount as a proportion of data (\texttt{q}), and benchmark value to extract high density hexagons (\texttt{benchmark\_highdens}). The function returns an object that includes the scaled NLDR object (\texttt{nldr\_obj}), the hexagonal object (\texttt{hb\_obj}), the fitted model in both \(2\text{-}D\) (\texttt{model\_2d}), and \(p\text{-}D\) (\texttt{model\_highd}), and triangular mesh (\texttt{trimesh\_data}).

\begin{verbatim}
fit_highd_model(
  highd_data = scurve, 
  nldr_data = scurve_umap, 
  bin1 = 15, 
  q = 0.1, 
  benchmark_highdens = 5)
\end{verbatim}

\begin{verbatim}
#> $nldr_obj
#> $nldr_obj$scaled_nldr
#> # A tibble: 5,000 x 3
#>     emb1  emb2    ID
#>    <dbl> <dbl> <int>
#>  1 0.707 0.839     1
#>  2 0.231 0.401     2
#>  3 0.232 0.215     3
#>  4 0.790 0.564     4
#>  5 0.761 0.551     5
#>  6 0.445 0.721     6
#>  7 0.900 0.137     7
#>  8 0.247 0.392     8
#>  9 0.325 0.542     9
#> 10 0.278 0.231    10
#> # i 4,990 more rows
#> 
#> $nldr_obj$lim1
#> [1] -14.42166  13.32655
#> 
#> $nldr_obj$lim2
#> [1] -12.43687  12.32455
#> 
#> 
#> $hb_obj
#> $a1
#> [1] 0.08326136
#> 
#> $a2
#> [1] 0.07210645
#> 
#> $bins
#> [1] 15 16
#> 
#> $start_point
#> [1] -0.10000000 -0.08923607
#> 
#> $centroids
#> # A tibble: 240 x 3
#>    hexID     c_x     c_y
#>    <int>   <dbl>   <dbl>
#>  1     1 -0.1    -0.0892
#>  2     2 -0.0167 -0.0892
#>  3     3  0.0665 -0.0892
#>  4     4  0.150  -0.0892
#>  5     5  0.233  -0.0892
#>  6     6  0.316  -0.0892
#>  7     7  0.400  -0.0892
#>  8     8  0.483  -0.0892
#>  9     9  0.566  -0.0892
#> 10    10  0.649  -0.0892
#> # i 230 more rows
#> 
#> $hex_poly
#> # A tibble: 1,440 x 3
#>    hex_poly_id       x       y
#>          <int>   <dbl>   <dbl>
#>  1           1 -0.1    -0.0412
#>  2           1 -0.142  -0.0652
#>  3           1 -0.142  -0.113 
#>  4           1 -0.1    -0.137 
#>  5           1 -0.0584 -0.113 
#>  6           1 -0.0584 -0.0652
#>  7           2 -0.0167 -0.0412
#>  8           2 -0.0584 -0.0652
#>  9           2 -0.0584 -0.113 
#> 10           2 -0.0167 -0.137 
#> # i 1,430 more rows
#> 
#> $data_hb_id
#> # A tibble: 5,000 x 4
#>     emb1  emb2    ID hexID
#>    <dbl> <dbl> <int> <int>
#>  1 0.707 0.839     1   205
#>  2 0.231 0.401     2   109
#>  3 0.232 0.215     3    65
#>  4 0.790 0.564     4   146
#>  5 0.761 0.551     5   146
#>  6 0.445 0.721     6   172
#>  7 0.900 0.137     7    58
#>  8 0.247 0.392     8   110
#>  9 0.325 0.542     9   141
#> 10 0.278 0.231    10    80
#> # i 4,990 more rows
#> 
#> $std_cts
#> # A tibble: 142 x 3
#>    hexID bin_counts std_counts
#>    <int>      <int>      <dbl>
#>  1    21         12     0.16  
#>  2    22         17     0.227 
#>  3    23         22     0.293 
#>  4    24         10     0.133 
#>  5    25          7     0.0933
#>  6    26         12     0.16  
#>  7    27          1     0.0133
#>  8    36         42     0.56  
#>  9    37         42     0.56  
#> 10    38         44     0.587 
#> # i 132 more rows
#> 
#> $tot_bins
#> [1] 240
#> 
#> $non_bins
#> [1] 142
#> 
#> $pts_bins
#> # A tibble: 142 x 2
#>    hexID pts_list  
#>    <int> <list>    
#>  1    21 <int [12]>
#>  2    22 <int [17]>
#>  3    23 <int [22]>
#>  4    24 <int [10]>
#>  5    25 <int [7]> 
#>  6    26 <int [12]>
#>  7    27 <int [1]> 
#>  8    36 <int [42]>
#>  9    37 <int [42]>
#> 10    38 <int [44]>
#> # i 132 more rows
#> 
#> attr(,"class")
#> [1] "hex_bin_obj"
#> 
#> $model_highd
#> # A tibble: 130 x 8
#>    hexID      x1    x2    x3         x4        x5       x6        x7
#>    <int>   <dbl> <dbl> <dbl>      <dbl>     <dbl>    <dbl>     <dbl>
#>  1    21 -0.992   1.91 1.11  -0.000427   0.000624  0.00749  0.00105 
#>  2    22 -0.906   1.93 1.41  -0.0000183  0.00331  -0.0204  -0.000363
#>  3    23 -0.680   1.93 1.72  -0.000810  -0.00259  -0.00449  0.00153 
#>  4    24 -0.272   1.93 1.96   0.00251    0.00668  -0.0460   0.00128 
#>  5    25  0.0760  1.93 2.00   0.00876    0.00447   0.00851 -0.00195 
#>  6    26  0.461   1.93 1.89  -0.00478    0.00492   0.00835  0.00172 
#>  7    36 -0.985   1.75 0.853 -0.00202    0.000397  0.00331  0.000338
#>  8    37 -0.980   1.66 1.17  -0.000374  -0.00154   0.0165   0.000126
#>  9    38 -0.821   1.64 1.56  -0.000459   0.000538 -0.0123   0.000780
#> 10    39 -0.484   1.68 1.87   0.00313    0.00241   0.00823 -0.00117 
#> # i 120 more rows
#> 
#> $model_2d
#> # A tibble: 130 x 5
#>    hexID   c_x     c_y bin_counts std_counts
#>    <int> <dbl>   <dbl>      <dbl>      <dbl>
#>  1    21 0.358 -0.0171         12     0.16  
#>  2    22 0.441 -0.0171         17     0.227 
#>  3    23 0.524 -0.0171         22     0.293 
#>  4    24 0.608 -0.0171         10     0.133 
#>  5    25 0.691 -0.0171          7     0.0933
#>  6    26 0.774 -0.0171         12     0.16  
#>  7    36 0.316  0.0550         42     0.56  
#>  8    37 0.400  0.0550         42     0.56  
#>  9    38 0.483  0.0550         44     0.587 
#> 10    39 0.566  0.0550         39     0.52  
#> # i 120 more rows
#> 
#> $trimesh_data
#> # A tibble: 322 x 8
#>     from    to x_from  y_from  x_to   y_to from_count to_count
#>    <int> <int>  <dbl>   <dbl> <dbl>  <dbl>      <dbl>    <dbl>
#>  1    16    26 0.275   0.127  0.233 0.199          65       67
#>  2    25    26 0.150   0.199  0.233 0.199          39       67
#>  3    25    37 0.150   0.199  0.191 0.271          39       62
#>  4    36    49 0.108   0.271  0.150 0.343          62       34
#>  5    36    37 0.108   0.271  0.191 0.271          62       62
#>  6     1     7 0.358  -0.0171 0.316 0.0550         12       42
#>  7    48    49 0.0665  0.343  0.150 0.343          45       34
#>  8    37    38 0.191   0.271  0.275 0.271          62       68
#>  9    26    27 0.233   0.199  0.316 0.199          67       56
#> 10    27    38 0.316   0.199  0.275 0.271          56       68
#> # i 312 more rows
\end{verbatim}

\subsection{\texorpdfstring{Constructing the \(2\text{-}D\) Model}{Constructing the 2\textbackslash text\{-\}D Model}}\label{constructing-the-2text-d-model}

Constructing the \(2\text{-}D\) model primarily involves (i) scaling the NLDR data, (ii) binning the data, (iii) obtaining bin centroids, (iv) connecting centroids with line segments to indicate neighbors, and (v) Remove low-density hexagons.

\subsubsection{Scaling the Data}\label{scaling-the-data}

The algorithm starts by scaling the NLDR data to a standard range using the \texttt{gen\_scaled\_data()} function. This function standardizes the data so that the first embedding ranges from \(0\) to \(1\), while the second embedding scales from \(0\) to the maximum value of the second embedding. The output includes the scaled NLDR data along with the original limits of the embeddings.

\begin{verbatim}
scurve_umap_obj <- gen_scaled_data(nldr_data = scurve_umap)

scurve_umap_obj
\end{verbatim}

\begin{verbatim}
#> $scaled_nldr
#> # A tibble: 5,000 x 3
#>     emb1  emb2    ID
#>    <dbl> <dbl> <int>
#>  1 0.707 0.839     1
#>  2 0.231 0.401     2
#>  3 0.232 0.215     3
#>  4 0.790 0.564     4
#>  5 0.761 0.551     5
#>  6 0.445 0.721     6
#>  7 0.900 0.137     7
#>  8 0.247 0.392     8
#>  9 0.325 0.542     9
#> 10 0.278 0.231    10
#> # i 4,990 more rows
#> 
#> $lim1
#> [1] -14.42166  13.32655
#> 
#> $lim2
#> [1] -12.43687  12.32455
\end{verbatim}

\subsubsection{Computing hexagon grid configurations}\label{computing-hexagon-grid-configurations}

The configurations of a hexagonal grid are determined by the number of bins and the bin width in each direction. The function \texttt{calc\_bins\_y()} is used for this purpose. This function accepts an object containing scaled NLDR data in the first and second columns, along with numeric vectors that represent the limits of the original NLDR data, the number of bins along the x-axis (\texttt{bin1}), and the buffer amount as a proportion.

\begin{verbatim}
bin_configs <- calc_bins_y(
  nldr_obj = scurve_umap_obj, 
  bin1 = 15, 
  q = 0.1)

bin_configs
\end{verbatim}

\begin{verbatim}
#> $bin2
#> [1] 16
#> 
#> $a1
#> [1] 0.08326136
#> 
#> $a2
#> [1] 0.07210645
\end{verbatim}

\subsubsection{Binning the data}\label{binning-the-data}

Points are allocated to bins based on the nearest centroid. The hexagonal binning algorithm can be executed using the \texttt{hex\_binning()} function, or its components can be run separately for added flexibility. The parameters used within \texttt{hex\_binning()} include an object containing scaled NLDR data in the first and second columns, along with numeric vectors that represent the limits of the original NLDR data (\texttt{nldr\_obj}), the number of bins along the x-axis (\texttt{bin1}), and the buffer amount as a proportion of the data (\texttt{q}). The output is an object of the \texttt{hex\_bin\_obj} class, which contains the bin widths in each direction (\texttt{a1}, \texttt{a2}), the number of bins in each direction (\texttt{bins}), the coordinates of the hexagonal grid starting point (\texttt{start\_point}), the details of bin centroids (\texttt{centroids}), the coordinates of bins (\texttt{hex\_poly}), NLDR components with their corresponding hexagon IDs (\texttt{data\_hb\_id}), hex bins with their corresponding standardized counts (\texttt{std\_cts}), the total number of bins (\texttt{tot\_bins}), the number of non-empty bins (\texttt{non\_bins}), and the points within each hexagon (\texttt{pts\_bins}).

\begin{verbatim}
hb_obj <- hex_binning(
  nldr_obj = scurve_umap_obj, 
  bin1 = 15, 
  q = 0.1)
\end{verbatim}

\begin{figure}
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/fig-hex-param-1} \caption{The components of the hexagon grid illustrating notation.}\label{fig:fig-hex-param}
\end{figure}

\begin{verbatim}
all_centroids_df <- gen_centroids(
  nldr_obj = scurve_umap_obj, 
  bin1 = 15, 
  q = 0.1
  )

all_centroids_df
\end{verbatim}

\begin{verbatim}
#> # A tibble: 240 x 3
#>    hexID     c_x     c_y
#>    <int>   <dbl>   <dbl>
#>  1     1 -0.1    -0.0892
#>  2     2 -0.0167 -0.0892
#>  3     3  0.0665 -0.0892
#>  4     4  0.150  -0.0892
#>  5     5  0.233  -0.0892
#>  6     6  0.316  -0.0892
#>  7     7  0.400  -0.0892
#>  8     8  0.483  -0.0892
#>  9     9  0.566  -0.0892
#> 10    10  0.649  -0.0892
#> # i 230 more rows
\end{verbatim}

\begin{verbatim}
all_hex_coord <- gen_hex_coord(
  centroids_data = all_centroids_df, 
  a1 = bin_configs$a1
  )

all_hex_coord
\end{verbatim}

\begin{verbatim}
#> # A tibble: 1,440 x 3
#>    hex_poly_id       x       y
#>          <int>   <dbl>   <dbl>
#>  1           1 -0.1    -0.0412
#>  2           1 -0.142  -0.0652
#>  3           1 -0.142  -0.113 
#>  4           1 -0.1    -0.137 
#>  5           1 -0.0584 -0.113 
#>  6           1 -0.0584 -0.0652
#>  7           2 -0.0167 -0.0412
#>  8           2 -0.0584 -0.0652
#>  9           2 -0.0584 -0.113 
#> 10           2 -0.0167 -0.137 
#> # i 1,430 more rows
\end{verbatim}

\begin{verbatim}
umap_hex_id <- assign_data(
  nldr_obj = scurve_umap_obj, 
  centroids_data = all_centroids_df
  )

umap_hex_id
\end{verbatim}

\begin{verbatim}
#> # A tibble: 5,000 x 4
#>     emb1  emb2    ID hexID
#>    <dbl> <dbl> <int> <int>
#>  1 0.707 0.839     1   205
#>  2 0.231 0.401     2   109
#>  3 0.232 0.215     3    65
#>  4 0.790 0.564     4   146
#>  5 0.761 0.551     5   146
#>  6 0.445 0.721     6   172
#>  7 0.900 0.137     7    58
#>  8 0.247 0.392     8   110
#>  9 0.325 0.542     9   141
#> 10 0.278 0.231    10    80
#> # i 4,990 more rows
\end{verbatim}

\begin{verbatim}
std_df <- compute_std_counts(
  scaled_nldr_hexid = umap_hex_id
  )

std_df
\end{verbatim}

\begin{verbatim}
#> # A tibble: 142 x 3
#>    hexID bin_counts std_counts
#>    <int>      <int>      <dbl>
#>  1    21         12     0.16  
#>  2    22         17     0.227 
#>  3    23         22     0.293 
#>  4    24         10     0.133 
#>  5    25          7     0.0933
#>  6    26         12     0.16  
#>  7    27          1     0.0133
#>  8    36         42     0.56  
#>  9    37         42     0.56  
#> 10    38         44     0.587 
#> # i 132 more rows
\end{verbatim}

\begin{verbatim}
pts_df <- find_pts(
  scaled_nldr_hexid = umap_hex_id
  )

pts_df
\end{verbatim}

\begin{verbatim}
#> # A tibble: 142 x 2
#>    hexID pts_list  
#>    <int> <list>    
#>  1    21 <int [12]>
#>  2    22 <int [17]>
#>  3    23 <int [22]>
#>  4    24 <int [10]>
#>  5    25 <int [7]> 
#>  6    26 <int [12]>
#>  7    27 <int [1]> 
#>  8    36 <int [42]>
#>  9    37 <int [42]>
#> 10    38 <int [44]>
#> # i 132 more rows
\end{verbatim}

\subsubsection{Obtaining bin centroids}\label{obtaining-bin-centroids}

\begin{verbatim}
df_bin_centroids <- extract_hexbin_centroids(
  centroids_data = all_centroids_df, 
  counts_data = hb_obj$std_cts
  )

df_bin_centroids
\end{verbatim}

\begin{verbatim}
#> # A tibble: 240 x 5
#>    hexID     c_x     c_y bin_counts std_counts
#>    <int>   <dbl>   <dbl>      <dbl>      <dbl>
#>  1     1 -0.1    -0.0892          0          0
#>  2     2 -0.0167 -0.0892          0          0
#>  3     3  0.0665 -0.0892          0          0
#>  4     4  0.150  -0.0892          0          0
#>  5     5  0.233  -0.0892          0          0
#>  6     6  0.316  -0.0892          0          0
#>  7     7  0.400  -0.0892          0          0
#>  8     8  0.483  -0.0892          0          0
#>  9     9  0.566  -0.0892          0          0
#> 10    10  0.649  -0.0892          0          0
#> # i 230 more rows
\end{verbatim}

\subsubsection{Indicating neighbors by line segments connecting centroids}\label{indicating-neighbors-by-line-segments-connecting-centroids}

To indicate neighbors, the \texttt{tri\_bin\_centroids()} function is used to triangulate bin centroids. Following this, \texttt{gen\_edges()} function computes the line segments that connect neighboring bins by providing the triangulated data. This results the coordinates that generate the connecting lines.

\begin{verbatim}
tr_object <- tri_bin_centroids(
  centroids_data = df_bin_centroids
  )

trimesh <- gen_edges(tri_object = tr_object, a1 = hb_obj$a1)
trimesh
\end{verbatim}

\begin{verbatim}
#> # A tibble: 653 x 8
#>     from    to  x_from  y_from    x_to    y_to from_count to_count
#>    <int> <int>   <dbl>   <dbl>   <dbl>   <dbl>      <dbl>    <dbl>
#>  1     1     2 -0.1    -0.0892 -0.0167 -0.0892          0        0
#>  2    16    17 -0.0584 -0.0171  0.0249 -0.0171          0        0
#>  3    16    32 -0.0584 -0.0171 -0.0167  0.0550          0        0
#>  4     3    17  0.0665 -0.0892  0.0249 -0.0171          0        0
#>  5    17    18  0.0249 -0.0171  0.108  -0.0171          0        0
#>  6    17    33  0.0249 -0.0171  0.0665  0.0550          0        0
#>  7    31    46 -0.1     0.0550 -0.0584  0.127           0        0
#>  8    32    47 -0.0167  0.0550  0.0249  0.127           0        0
#>  9    32    33 -0.0167  0.0550  0.0665  0.0550          0        0
#> 10     4    18  0.150  -0.0892  0.108  -0.0171          0        0
#> # i 643 more rows
\end{verbatim}

In some cases, distant centroids may be connected, resulting in long line segments that can affect the smoothness of the \(2\text{-}D\) representation. To address this issue, the \texttt{find\_lg\_benchmark()} function is used. This function computes a threshold based on the distances of line segments, determining when long edges should be removed.

\subsubsection{Remove low-density hexagons}\label{remove-low-density-hexagons}

In certain scenarios, hexagonal bins may contain a few number of points. To ensure comprehensive coverage of NLDR data, it is important to select hexagonal bins with a suitable number of data points. The \texttt{find\_low\_dens\_hex()} function identifies hexagons with low point densities, considering the densities of their neighboring bins as well. Users can initially identify low-density hexagons and then use this function to evaluate how removing them might affect the model fit by examining their neighbors.

\begin{verbatim}
find_low_dens_hex(
  centroids_data = df_bin_centroids, 
  bin1 = 15, 
  benchmark_mean_dens = 0.05
)
\end{verbatim}

\begin{verbatim}
#>  [1]   1   2   3   4   5  11  12  13  14  15  16  17  18  19  30  31  32  33  46
#> [20]  47  61  75  76  90 105 120 135 150 151 166 181 182 196 197 198 211 212 213
#> [39] 214 215 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240
\end{verbatim}

\subsection{Lifting the model into high dimensions}\label{lifting-the-model-into-high-dimensions}

The final step involves lifting the fitted \(2\text{-}D\) model into \(p\text{-}D\) by computing the \(p\text{-}D\) mean of data points within each hexgaonal bin to represent bin centroids. This transformation is performed using the \texttt{avg\_highd\_data()} function, which takes \(p\text{-}D\) data (\texttt{highd\_data}) and embedding data with their corresponding hexagonal bin IDs as inputs (\texttt{scaled\_nldr\_hexid}).

\begin{verbatim}
df_bin <- avg_highd_data(
  highd_data = scurve, 
  scaled_nldr_hexid = hb_obj$data_hb_id
)

df_bin
\end{verbatim}

\begin{verbatim}
#> # A tibble: 142 x 8
#>    hexID      x1    x2    x3         x4        x5       x6        x7
#>    <int>   <dbl> <dbl> <dbl>      <dbl>     <dbl>    <dbl>     <dbl>
#>  1    21 -0.992   1.91 1.11  -0.000427   0.000624  0.00749  0.00105 
#>  2    22 -0.906   1.93 1.41  -0.0000183  0.00331  -0.0204  -0.000363
#>  3    23 -0.680   1.93 1.72  -0.000810  -0.00259  -0.00449  0.00153 
#>  4    24 -0.272   1.93 1.96   0.00251    0.00668  -0.0460   0.00128 
#>  5    25  0.0760  1.93 2.00   0.00876    0.00447   0.00851 -0.00195 
#>  6    26  0.461   1.93 1.89  -0.00478    0.00492   0.00835  0.00172 
#>  7    27  0.719   1.99 1.70   0.0109    -0.00349  -0.0297  -0.00223 
#>  8    36 -0.985   1.75 0.853 -0.00202    0.000397  0.00331  0.000338
#>  9    37 -0.980   1.66 1.17  -0.000374  -0.00154   0.0165   0.000126
#> 10    38 -0.821   1.64 1.56  -0.000459   0.000538 -0.0123   0.000780
#> # i 132 more rows
\end{verbatim}

\subsection{Prediction}\label{prediction}

The \texttt{predict\_emb()} function is used to predict \(2\text{-}D\) embedding for a new \(p\text{-}D\) data point using the fitted model. This function is useful to predict \(2\text{-}D\) embedding irrespective of the NLDR technique.

In the prediction process, first, the nearest \(p\text{-}D\) model point is identified for a given new \(p\text{-}D\) data point by computing \(p\text{-}D\) Euclidean distance. Then, the corresponding \(2\text{-}D\) bin centroid mapping for the identified \(p\text{-}D\) model point is determined. Finally, the coordinates of the identified \(2\text{-}D\) bin centroid is used as the predicted NLDR embedding for the new \(p\text{-}D\) data point.

\begin{verbatim}
predict_emb(
  highd_data = scurve, 
  model_2d = df_bin_centroids, 
  model_highd = df_bin
  )
\end{verbatim}

\begin{verbatim}
#> # A tibble: 5,000 x 4
#>    pred_emb_1 pred_emb_2    ID pred_hb_id
#>         <dbl>      <dbl> <int>      <int>
#>  1      0.691      0.848     1        205
#>  2      0.191      0.416     2        109
#>  3      0.316      0.199     3         66
#>  4      0.774      0.560     4        146
#>  5      0.774      0.560     5        146
#>  6      0.441      0.704     6        172
#>  7      0.941      0.127     7         58
#>  8      0.275      0.416     8        110
#>  9      0.358      0.560     9        141
#> 10      0.316      0.343    10         96
#> # i 4,990 more rows
\end{verbatim}

\subsection{Compute residuals and Mean Square Error (MSE)}\label{compute-residuals-and-mean-square-error-mse}

As a Goodness of fit statistics for the model, \texttt{glance()} is used to compute residuals and MSE. These metrics are used to assess how well the fitted model will capture the underlying structure of the \(p\text{-}D\) data.

\begin{verbatim}
glance(
  highd_data = scurve, 
  model_2d = df_bin_centroids, 
  model_highd = df_bin
  )
\end{verbatim}

\begin{verbatim}
#> # A tibble: 1 x 2
#>   Error    MSE
#>   <dbl>  <dbl>
#> 1 1481. 0.0325
\end{verbatim}

Furthermore, \texttt{augment()} accepts \(2\text{-}D\) and \(p\text{-}D\) model points, and the \(p\text{-}D\) data and adds information about each observation in the data set. Most commonly, this includes predicted values, residuals, row wise total error, absolute error for the fitted values, and row wise total absolute error.

Users can pass data to \texttt{augment()} via either the \texttt{training\_data} argument or the \texttt{newdata} argument. If data is passed to the \texttt{training\_data} argument, it must be exactly the data that was used to fit the model. Alternatively, datasets can be passed to \texttt{newdata} to augment data that was not used during model fitting. This requires that at least all predictor variable columns used to fit the model are present. If the original outcome variable used to fit the model is not included in \texttt{newdata}, then no corresponding column will be included in the output.

The augmented dataset is always returned as a \texttt{tibble::tibble} with the same number of rows as the passed dataset.

\begin{verbatim}
model_error <- augment(
  highd_data = scurve, 
  model_2d = df_bin_centroids, 
  model_highd = df_bin
  )

model_error
\end{verbatim}

\begin{verbatim}
#> # A tibble: 5,000 x 32
#>       ID      x1     x2       x3       x4       x5       x6        x7 pred_hb_id
#>    <int>   <dbl>  <dbl>    <dbl>    <dbl>    <dbl>    <dbl>     <dbl>      <int>
#>  1     1 -0.120  1.64   -1.99     0.0104   0.0125   0.0923  -0.00128         205
#>  2     2 -0.0492 1.51    0.00121 -0.0177   0.00726 -0.0362  -0.00535         109
#>  3     3 -0.774  1.30    0.367   -0.00173  0.0156  -0.0962   0.00335          66
#>  4     4 -0.606  0.246  -1.80    -0.00897 -0.0187  -0.0716   0.00126         146
#>  5     5 -0.478  0.0177 -1.88     0.00848  0.00533  0.0998   0.000677        146
#>  6     6  0.818  0.927  -1.58    -0.00318 -0.00980  0.0989   0.00696         172
#>  7     7  0.910  1.40    1.42     0.00699 -0.0182  -0.0710   0.00966          58
#>  8     8 -0.0691 1.59    0.00239  0.0127  -0.0130   0.0396  -0.000185        110
#>  9     9  0.859  1.59   -0.488   -0.0119   0.00421 -0.00440 -0.00595         141
#> 10    10 -0.727  1.62    0.314    0.00251  0.0177  -0.0755  -0.00369          96
#> # i 4,990 more rows
#> # i 23 more variables: model_high_d_x1 <dbl>, model_high_d_x2 <dbl>,
#> #   model_high_d_x3 <dbl>, model_high_d_x4 <dbl>, model_high_d_x5 <dbl>,
#> #   model_high_d_x6 <dbl>, model_high_d_x7 <dbl>, error_square_x1 <dbl>,
#> #   error_square_x2 <dbl>, error_square_x3 <dbl>, error_square_x4 <dbl>,
#> #   error_square_x5 <dbl>, error_square_x6 <dbl>, error_square_x7 <dbl>,
#> #   row_wise_total_error <dbl>, abs_error_x1 <dbl>, abs_error_x2 <dbl>, ...
\end{verbatim}

\subsection{Visualizations}\label{visualizations}

The package provides five basic visualizations which includes one to visualize the full hexagonal grid in \(2\text{-}D\), three visualizations related to the \(2\text{-}D\) model (static visualizations), and one related to the \(p\text{-}D\) model (dynamic visualization). Each visualization can be generated using its respective function, as described in this section.

\subsubsection{\texorpdfstring{\(2\text{-}D\) model visualization}{2\textbackslash text\{-\}D model visualization}}\label{text-d-model-visualization}

The \texttt{geom\_hexgrid()} function is used to plot the hexagonal grid from the provided centroid data set.

\begin{verbatim}
ggplot() + 
  geom_hexgrid(
    data = hb_obj$centroids, 
    aes(x = c_x, y = c_y)
    ) 
\end{verbatim}

\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/unnamed-chunk-19-1}

To visualize the \(2\text{-}D\) model, mainly three functions are used. As shown in Figure \ref{fig:mesh-plots}a, \texttt{geom\_trimesh()} to visualize the triangular mesh by adding a new layer to \texttt{ggplot()}. After identifying benchmark value to remove long edge, \texttt{vis\_lg\_mesh()} is used to visualize the triangular mesh by coloring the small and long edges. As shown in Figure \ref{fig:mesh-plots}b, the small and long edges are colored by black and red respectively. Following this, \texttt{vis\_rmlg\_mesh()} is used to visualize smoothed \(2\text{-}D\) model which is the \(2\text{-}D\) model after removing the long edges (see Figure \ref{fig:mesh-plots}c). In \texttt{vis\_lg\_mesh()} and \texttt{vis\_rmlg\_mesh()}, \texttt{benchmark\_value} argument controls the edge removal in \(2\text{-}D\). Using small value of \texttt{benchmark\_value}, will produce a triangular mesh with missing data structure; for example \texttt{benchmark\_value\ =\ 0.3} shows two clusters rather than continuous structure, while \texttt{benchmark\_value\ =\ 0.8} creates long edges and mislead the data structure in \(p\text{-}D\) space.

\begin{verbatim}
ggplot() + 
  geom_trimesh(
    data = hb_obj$centroids, 
    aes(x = c_x, y = c_y)
    ) 
\end{verbatim}

\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/unnamed-chunk-20-1}

\subsubsection{\texorpdfstring{\(p\text{-}D\) model visualization}{p\textbackslash text\{-\}D model visualization}}\label{ptext-d-model-visualization}

Displaying the \(p\text{-}D\) model overlaid on the data is done using the function \texttt{show\_langevitour()}. This visualization is helpful for visually evaluating how well the model fits the data. The function requires several arguments: data along with their corresponding hexagonal bin ID, \(2\text{-}D\) and \(p\text{-}D\) model points, the threshold for removing long edges, and the distance data set.

\begin{verbatim}
df_bin_centroids <- df_bin_centroids |>
  dplyr::filter(bin_counts > 10)

df_exe <- comb_data_model(
  highd_data = scurve, 
  model_highd = df_bin, 
  model_2d = df_bin_centroids
  )

df_exe
\end{verbatim}

\begin{verbatim}
#> # A tibble: 5,122 x 8
#>         x1    x2    x3         x4        x5       x6        x7 type 
#>      <dbl> <dbl> <dbl>      <dbl>     <dbl>    <dbl>     <dbl> <chr>
#>  1 -0.992   1.91 1.11  -0.000427   0.000624  0.00749  0.00105  model
#>  2 -0.906   1.93 1.41  -0.0000183  0.00331  -0.0204  -0.000363 model
#>  3 -0.680   1.93 1.72  -0.000810  -0.00259  -0.00449  0.00153  model
#>  4  0.461   1.93 1.89  -0.00478    0.00492   0.00835  0.00172  model
#>  5 -0.985   1.75 0.853 -0.00202    0.000397  0.00331  0.000338 model
#>  6 -0.980   1.66 1.17  -0.000374  -0.00154   0.0165   0.000126 model
#>  7 -0.821   1.64 1.56  -0.000459   0.000538 -0.0123   0.000780 model
#>  8 -0.484   1.68 1.87   0.00313    0.00241   0.00823 -0.00117  model
#>  9 -0.0991  1.70 1.99   0.00103    0.00150   0.00877 -0.000193 model
#> 10  0.295   1.74 1.95  -0.00165    0.000459  0.00330  0.000257 model
#> # i 5,112 more rows
\end{verbatim}

\begin{verbatim}
trimesh <- trimesh |>
  dplyr::filter(from_count > 10,
                to_count > 10)

trimesh <- update_trimesh_index(trimesh)

show_langevitour(
  point_data = df_exe, 
  edge_data = trimesh
  )
\end{verbatim}

\subsubsection{Link plots}\label{link-plots}

There are mainly two interactive link plots can be generated. \texttt{show\_link\_plots()} helps to examine the fit. The function requires several arguments: points data which contain Non-linear dimension reduction data, high-dimensional data, and high-dimensional model data, and edge data where the from and to links of the edges.

\texttt{show\_error\_link\_plots()} helps to see investigate whether the model fits the points everywhere or fits better in some places, or simply mismatches the pattern. The function requires several arguments: points data which contain Non-linear dimension reduction data, high-dimensional data, high-dimensional model data, and model error, and edge data where the from and to links of the edges.

\begin{verbatim}
df_exe <- comb_all_data_model(
  highd_data = scurve, 
  nldr_data = scurve_umap, 
  model_highd = df_bin, 
  model_2d = df_bin_centroids
  )
\end{verbatim}

\begin{verbatim}
show_link_plots(
  point_data = df_exe, 
  edge_data = trimesh
  )
\end{verbatim}

\begin{verbatim}
df_exe <- comb_all_data_model_error(
  highd_data = scurve, 
  nldr_data = scurve_umap, 
  model_highd = df_bin, 
  model_2d = df_bin_centroids, 
  error_data = model_error
  )
\end{verbatim}

\begin{verbatim}
show_error_link_plots(
  point_data = df_exe, 
  edge_data = trimesh
  )
\end{verbatim}

\subsection{Find the most appropriate fit}\label{find-the-most-appropriate-fit}

\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/unnamed-chunk-28-1}

\section{Application}\label{application}

Single-cell RNA sequencing (scRNA-seq) is a popular and powerful technology that allows you to profile the whole transcriptome of a large number of individual cells \citep{andrews2021}.

Clustering of single-cell data is used to identify groups of cells with similar expression profiles. NLDR often used to summarise the discovered clusters, and help to understand the results. The purpose of this example is to \emph{illustrate how to use our method to help decide on an appropriate NLDR layout that accurately represents the data}.

Limb muscle cells of mice in \citet{tabula2018} are examined. There are \(1067\) single cells, with \(14997\) gene expressions. Following their pre-processing, tSNE was performed using ten principal components. \ref{fig:nldr-layouts} (b) is the reproduction of the published plot. The question is whether this accurately represents the cluster structure in the data. Our method can help here, and also help to provide a better \(2-\text{D}\) layout, as needed.

\begin{figure}
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/limb-rmse-1} \caption{Assessing which of the 6 NLDR layouts on the limb muscle data  (shown in \\ref{fig:nldr-layouts}) is the better representation using RMSE for varying binwidth ($a_1$). Colour  used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-f). Layout d is perform well at large binwidth (where the binwidth is not enough to capture the data struture) and poorly as bin width decreases. Layout f is the best choice.}\label{fig:limb-rmse}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/model-limb-1} \caption{Compare the published $2-\text{D}$ layout (\\ref{fig:nldr-layouts} b) and the $2-\text{D}$ layout selected (\\ref{fig:nldr-layouts} f) by RMSE plot (\\ref{fig:limb-rmse}) from the tSNE, UMAP, PHATE, TriMAP, and PaCMAP with different (hyper)parameters. The Limb muscle data ($n =  1067$) has seven close different shaped clusters in $10\text{-}D$.}\label{fig:model-limb}
\end{figure}

\section{Discussion}\label{discussion}

This paper presents the R package \texttt{quollr} to develop a way to take the fitted model, as represented by the positions of points in \(2\text{-}D\), and turn it into a high-dimensional wireframe to overlay on the data, viewing it with a tour.

The paper includes a clustering example to illustrate how \texttt{quollr} is useful to assess which NLDR technique and which (hyper)parameter choice gives the most accurate representation. In addition, how to select parameters for hexagonal binning and fitting model are explained.

Possible future improvements would be\ldots{}

This new tool provides an effective start point for automatically creating regular hexagons and help to evaluate which NLDR technique and which hyperparameter choice gives the most accurate representation of \(p\text{-}D\) data.

\section{Acknowledgements}\label{acknowledgements}

This article is created using \CRANpkg{knitr} \citep{knitr} and \CRANpkg{rmarkdown} \citep{rmarkdown} in R with the \texttt{rjtools::rjournal\_article} template. The source code for reproducing this paper can be found at: \url{https://github.com/JayaniLakshika/paper-quollr}.

\bibliography{paper-quollr.bib}

\address{%
Jayani P. Gamage\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
\url{https://jayanilakshika.netlify.app/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-6265-6481}{0000-0002-6265-6481}}\\%
\email{jayani.piyadigamage@monash.edu}%
}

\address{%
Dianne Cook\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
\url{http://www.dicook.org/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3813-7155}{0000-0002-3813-7155}}\\%
\href{mailto:dicook@monash.edu}{\nolinkurl{dicook@monash.edu}}%
}

\address{%
Paul Harrison\\
Monash University\\%
MGBP, BDInstitute, VIC 3800 Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3980-268X}{0000-0002-3980-268X}}\\%
\href{mailto:paul.harrison@monash.edu}{\nolinkurl{paul.harrison@monash.edu}}%
}

\address{%
Michael Lydeamore\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0001-6515-827X}{0000-0001-6515-827X}}\\%
\href{mailto:michael.lydeamore@monash.edu}{\nolinkurl{michael.lydeamore@monash.edu}}%
}

\address{%
Thiyanga S. Talagala\\
University of Sri Jayewardenepura\\%
Department of Statistics, Gangodawila, Nugegoda 10100 Sri Lanka\\
%
\url{https://thiyanga.netlify.app/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-0656-9789}{0000-0002-0656-9789}}\\%
\href{mailto:ttalagala@sjp.ac.lk}{\nolinkurl{ttalagala@sjp.ac.lk}}%
}
