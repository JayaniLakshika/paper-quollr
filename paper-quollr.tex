% !TeX root = RJwrapper.tex
\title{quollr: An R Package for Visualizing 2-D Models from Non-linear Dimension Reductions in High Dimensional Space}


\author{by Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, and Thiyanga S. Talagala}

\maketitle

\abstract{%
Non-linear dimension reduction (NLDR) methods provide a low-dimensional representation of high-dimensional data (\(p\text{-}D\)) by applying a non-linear transformation. However, the complexity of the transformations and data structures can create wildly different representations depending on the method and hyper-parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures. The R package \CRANpkg{quollr} has been developed as a new visual tool to determine which method and which hyper-parameter choices provide the most accurate representation of high-dimensional data. The \texttt{scurve} data from the package is used to illustrate the algorithm. Single-cell RNA sequencing (scRNA-seq) data from mouse limb muscles are used to demonstrate the usability of the package.
}

\section{Introduction}\label{introduction}

Non-linear dimension reduction (NLDR) techniques, such as t-distributed stochastic neighbor embedding (tSNE) \citep{laurens2008}, uniform manifold approximation and projection (UMAP) \citep{leland2018}, potential of heat-diffusion for affinity-based trajectory embedding (PHATE) algorithm \citep{moon2019}, large-scale dimensionality reduction Using triplets (TriMAP) \citep{amid2019}, and pairwise controlled manifold approximation (PaCMAP) \citep{yingfan2021}, can create hugely different representations depending on the selected method and hyper-parameter choices. It is difficult to determine whether any of these representations are accurate, which one is the best, or whether they have missed important structures.

This paper presents the R package, \texttt{quollr}, which is useful for understanding how NLDR warps high-dimensional space and fits the data. Starting with an NLDR layout, our approach is to create a $2\text{-}D$ wireframe model representation, that can be lifted and displayed in the high-dimensional ($p\text{-}D$) space (Figure \ref{fig:overview}).

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/overview-1} \caption{Wireframe model representation of the NLDR layout, lifted and displayed in high-dimensional space. The left panel shows the NLDR layout with a triangular mesh overlay, forming the wireframe structure. This mesh can be lifted into higher dimensions and projected to examine how the geometric structure of the data is preserved. Panels (a1–a4) display different $2\text{-}D$projections of the lifted wireframe, where the underlying curved sheet structure of the data is more clearly visible. The triangulated mesh highlights how local neighborhoods in the layout correspond to relationships in the high-dimensional space, enabling diagnostics of distortion and preservation across dimensions.}\label{fig:overview}
\end{figure}

The paper is organized as follows. The next section introduces the implementation of the \texttt{quollr} package on CRAN, including a demonstration of the package's key functions and visualization capabilities. In the application section, we illustrate the algorithm's functionality for studying a clustering data structure. Finally, we conclude the paper with a brief summary and discuss potential opportunities for using our algorithm.

\section{Implementation}\label{implementation}

The implementation of \texttt{quollr} is designed to be modular, efficient, and easy to extend. The package is organized into a series of logical components that reflect the main stages of the workflow: data preprocessing, model fitting, low-density bin removal, prediction, visualization, and interactive exploration. This modular structure makes the code easier to maintain and allows new features to be added without disrupting the existing functionality.

\subsection{Software architecture}\label{software-architecture}

Internally, \texttt{quollr} is organized into five primary modules that correspond to key steps in the analysis pipeline:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data preparation} --- functions such as \texttt{scale\_highd()} and \texttt{prepare\_data()} standardize the input, manage variable naming, and ensure consistent identifiers across high-dimensional and embedded datasets.
\item
  \textbf{Hexagonal binning} --- the \texttt{hex\_bin\_data()} module constructs a two-dimensional hexagonal grid and aggregates local summaries of the high-dimensional data for each bin.
\item
  \textbf{Model fitting} --- the \texttt{fit\_highd\_model()} function computes high-dimensional centroids and pairwise bin connections to build a structured manifold representation.
\item
  \textbf{Error computation} --- functions like \texttt{compute\_errors()} and \texttt{aggregate\_bin\_error()} evaluate local and global discrepancies between the model and data, enabling diagnostic visualization.
\item
  \textbf{Visualization and linkage} --- plotting and tour functions (\texttt{show\_hex\_model()}, \texttt{show\_link\_plots()}, and \texttt{show\_error\_link\_plots()}) integrate outputs into linked static and dynamic displays using \texttt{ggplot2}, \texttt{langevitour}, and \texttt{crosstalk}.
\end{enumerate}

Each module is internally independent but communicates through standardized data objects (see next section).
This modular design simplifies maintenance and allows developers to extend individual components---such as substituting different binning strategies, embedding methods, or visualization tools---without altering the overall workflow.

\subsection{Data objects}\label{data-objects}

The internal data objects in \texttt{quollr} adhere to the \textbf{tidy data} principle, ensuring that every variable forms a column, every observation forms a row, and every type of observational unit forms a table.
This structure supports compatibility with the \texttt{tidyverse} and other R visualization frameworks.

The main object classes include:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{highd\_data}} -- a tibble containing the original high-dimensional observations with a unique identifier (\texttt{ID}) and variable columns prefixed with \texttt{"x"} (e.g., \texttt{x1}, \texttt{x2}, \ldots).
\item
  \textbf{\texttt{nldr\_data}} -- a tibble containing two-dimensional embeddings, labeled as \texttt{emb1} and \texttt{emb2}, matched to the same \texttt{ID}s.
\item
  \textbf{\texttt{highd\_vis\_model}} -- the primary model object returned by \texttt{fit\_highd\_model()}, containing two key components: the high-dimensional summaries for each bin and their corresponding two-dimensional positions.
\item
  \textbf{\texttt{hex\_bin\_obj}} -- the internal representation of the hexagonal grid, including bin coordinates (\texttt{a1}, \texttt{a2}), centroids, and normalized weights (\texttt{w\_h}).
\end{itemize}

These objects provide a coherent foundation for data transfer across \texttt{quollr}'s modules---from binning to model fitting and visualization---supporting reproducibility and transparency in analyses.

\subsection{Computational efficiency and optimization}\label{computational-efficiency-and-optimization}

Several core computations within \texttt{quollr} are optimized using compiled C++ code via the \texttt{Rcpp} and \texttt{RcppArmadillo} packages. While the user interacts with high-level R functions, performance-critical steps such as nearest-neighbor searches (\texttt{compute\_highd\_dist()}), error metrics (\texttt{compute\_errors()}), $2\text{-}D$ distance calculations (\texttt{calc\_2d\_dist\_cpp()}), and generation of hexagon coordinates (\texttt{gen\_hex\_coord\_cpp()}) are handled internally in C++. This design provides significant speedups when analyzing large datasets while maintaining a user-friendly R interface. These C++ functions are not exported but are bundled within the package and fully accessible for inspection in the source code.

\section{Usage}\label{usage}

The package is available on CRAN, and the development version is available at \url{https://jayanilakshika.github.io/quollr/}.

Our algorithm includes the following steps: (1) scaling the NLDR data, (2) computing configurations of a hexagon grid, (3) binning the data, (4) obtaining the centroids of each bin, (5) indicating neighboring bins with line segments that connect the centroids, and (6) lifting the model into high dimensions (Figure \ref{fig:algo-steps}). A detailed description of the algorithm can be found in \citet{gamage2025c}.

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/algo-steps-1} \caption{Key steps for constructing the model on the UMAP layout: (a) NLDR data, (b) hexagon bins, (c) bin centroids, (d) triangulated centroids, and (e) lifting the model into high dimensions. The `Scurve` data is shown.}\label{fig:algo-steps}
\end{figure}

The following demonstration of the package's functionality assumes \texttt{quollr} has been loaded. To begin the workflow, users need two inputs: the high-dimensional dataset and the corresponding nonlinear dimensionality reduction (NLDR) layout. The high-dimensional data must contain a unique \texttt{ID} column, with data columns prefixed by the letter \texttt{"x"} (e.g., \texttt{x1}, \texttt{x2}, etc.). The NLDR dataset should include embedding coordinates labeled as \texttt{emb1} and \texttt{emb2}, ensuring one-to-one correspondence with the high-dimensional data through the shared \texttt{ID}.

To illustrate the workflow, we use the built-in example dataset \texttt{scurve}, a \(7\text{-}D\) simulated dataset consisting of \(1000\) observations. The first three variables define a \(3\text{-}D\) S-shaped manifold, while the remaining four variables introduce low-magnitude uniform noise, yielding a structured yet noisy high-dimensional dataset.

\begin{verbatim}
data("scurve")
\end{verbatim}

For this example, we use the UMAP layout, which is produced using \texttt{n\_neighbors\ =} \(46\) and \texttt{min\_dist\ =} \(0.9\).

\begin{verbatim}
library(umap)
library(dplyr)

scurve_umap <- umap(
  scurve |> select(-ID),
  config = modifyList(umap.defaults, list(
    n_neighbors = 46,
    n_components = 2,
    min_dist = 0.9
  ))
)$layout |>
  as_tibble(.name_repair = ~ paste0("emb", 1:2)) |>
  mutate(ID = scurve$ID)
\end{verbatim}

\subsection{Main function}\label{main-function}

The mains steps for the algorithm can be executed by the main function \texttt{fit\_highd\_model()}, or can be run separately for more flexibility.

This function requires several parameters: the high-dimensional data (\texttt{highd\_data}), the emdedding data (\texttt{nldr\_data}), the number of bins along the x-axis (\texttt{b1}), the buffer amount as a proportion of data (\texttt{q}), and benchmark value to extract high density hexagons (\texttt{hd\_thresh}). The function returns an object of class \texttt{highd\_vis\_model} containing the scaled NLDR object (\texttt{nldr\_scaled\_obj}) with three elements: the first is the scaled NLDR data (\texttt{scaled\_nldr}), and the second and third are the limits of the original NLDR data (\texttt{lim1} and \texttt{lim2}); the hexagonal object (\texttt{hb\_obj}), the fitted model in both $2\text{-}D$ (\texttt{model\_2d}), and $p\text{-}D$ (\texttt{model\_highd}), and triangular mesh (\texttt{trimesh\_data}).

\begin{verbatim}
fit_highd_model(
  highd_data = scurve, 
  nldr_data = scurve_umap, 
  b1 = 21, 
  q = 0.1, 
  hd_thresh = 0)
\end{verbatim}

\subsection{\texorpdfstring{Constructing the \(2\text{-}D\) Model}{Constructing the 2\textbackslash text\{-\}D Model}}\label{constructing-the-2text-d-model}

Constructing the $2\text{-}D$ model primarily involves (i) scaling the NLDR data, (ii) binning the data, (iii) obtaining bin centroids, (iv) connecting centroids with line segments to indicate neighbors, and (v) removing low-density hexagons.

\subsubsection{Scaling the data}\label{scaling-the-data}

The algorithm starts by scaling the NLDR data to to the range \([0, 1] \times [0, y_{2,max}]\), where \(y_{2,max} = r_2/r_1\) is the ratio of ranges of embedding components. The output includes the scaled NLDR data (\texttt{scaled\_nldr}) along with the original limits of the embeddings (\texttt{lim1}, \texttt{lim2}).

\begin{verbatim}
scurve_umap_obj <- gen_scaled_data(nldr_data = scurve_umap)
\end{verbatim}

\subsubsection{Computing hexagon grid configuration}\label{computing-hexagon-grid-configuration}

The function \texttt{calc\_bins\_y()} determines the configuration of the hexagonal grid by computing the number of bins along the y-axis (\texttt{b2}), the hexagon width (\texttt{a1}), and height (\texttt{a2}). This function accepts (1) an object (\texttt{nldr\_scaled\_obj}) containing three elements: the first is the scaled NLDR data (\texttt{scaled\_nldr}), and the second and third are the limits of the original NLDR data (\texttt{lim1} and \texttt{lim2}); (2) the number of bins along the x-axis (\texttt{b1}), and (3) the buffer amount as a proportion (\texttt{q}). The buffer ensures that the grid fully covers the data space by extending one hexagon width (\(a_1\)) and height (\(a_2\)) beyond the observed data in all directions. By default, \(q = 0.1\), but it must be set to a value smaller than the minimum data value to avoid exceeding the data range.

\begin{verbatim}
bin_configs <- calc_bins_y(
  nldr_scaled_obj = scurve_umap_obj, 
  b1 = 25, 
  q = 0.1)

bin_configs
\end{verbatim}

\begin{verbatim}
> $b2
> [1] 36
> 
> $a1
> [1] 0.04943533
> 
> $a2
> [1] 0.04281225
\end{verbatim}

\subsubsection{Binning the data}\label{binning-the-data}

Points are allocated to bins based on the nearest centroid of the hexagonal bins. The hexagonal binning algorithm can be executed using the \texttt{hex\_binning()} function, or its individual components can be run separately for added flexibility. While running the process step by step would involve generating centroids, constructing hexagon coordinates, assigning points to bins, standardizing counts, and mapping the data back to hexagons, the \texttt{hex\_binning()} function automates this entire workflow. The parameters used within \texttt{hex\_binning()} are the object output from \texttt{gen\_scaled\_data} (\texttt{nldr\_scaled\_obj}); the number of bins along the x-axis (\texttt{b1}), and the buffer amount as a proportion of the data (\texttt{q}). The output is an object of the \texttt{hex\_bin\_obj} class, which contains the bin widths in each direction (\texttt{a1}, \texttt{a2}), the number of bins in each direction (\texttt{bins}), the coordinates of the hexagonal grid starting point (\texttt{start\_point}), the details of bin centroids (\texttt{centroids}), the coordinates of bins (\texttt{hex\_poly}), NLDR components with their corresponding hexagon IDs (\texttt{data\_hb\_id}), hex bins with their corresponding standardized counts (\texttt{std\_cts}), the total number of bins (\texttt{b}), the number of non-empty bins (\texttt{m}), and the points within each hexagon (\texttt{pts\_bins}).

\begin{verbatim}
hb_obj <- hex_binning(
  nldr_scaled_obj = scurve_umap_obj, 
  b1 = 25, 
  q = 0.1)
\end{verbatim}

\paragraph{Generating all possible centroids in a hexagonal grid}\label{generating-all-possible-centroids-in-a-hexagonal-grid}

The \texttt{gen\_centroids()} function calculates the centroids of a hexagonal grid.

The coordinate limits of the embedding (\texttt{lim1} and \texttt{lim2}) are used to compute the aspect ratio between the two axes, which informs vertical spacing. The function then calls \texttt{calc\_bins\_y()}, a helper function that determines the appropriate number of hexagons along y-axis (\texttt{b2}) and the width of each hexagon (\texttt{a1}) given the specified number of bins along the x-axis (\texttt{b1}) and buffer (\texttt{q}).

Then, the centroids are computed iteratively. The x-coordinates for centroids in odd-numbered rows are initialized as a sequence spaced by the hexagon width. Even-numbered rows are staggered by half this width to achieve a hexagonal tiling effect. Vertical spacing (\texttt{vs}) is given by \(\sqrt{3}/2 \times a_1\).

The y-coordinates for each row are similarly calculated, and paired with the x-coordinates based on whether the total number of rows is even or odd. In the case of an odd number of rows, the final row uses only the odd-row x-coordinates to maintain the alternating pattern.

Finally, a tibble is returned containing a unique hexagon ID (\texttt{h}) along with the corresponding x and y centroid coordinates (\texttt{c\_x}, \texttt{c\_y}), which define the layout of the hexagonal grid over the $2\text{-}D$ space.

\begin{verbatim}
all_centroids_df <- gen_centroids(
  nldr_scaled_obj = scurve_umap_obj, 
  b1 = 25, 
  q = 0.1
  )

all_centroids_df
\end{verbatim}

\begin{verbatim}
> # A tibble: 900 x 3
>        h      c_x    c_y
>    <int>    <dbl>  <dbl>
>  1     1 -0.1     -0.127
>  2     2 -0.0506  -0.127
>  3     3 -0.00113 -0.127
>  4     4  0.0483  -0.127
>  5     5  0.0977  -0.127
>  6     6  0.147   -0.127
>  7     7  0.197   -0.127
>  8     8  0.246   -0.127
>  9     9  0.295   -0.127
> 10    10  0.345   -0.127
> # i 890 more rows
\end{verbatim}

\paragraph{Creating the coordinates of the hexagons}\label{creating-the-coordinates-of-the-hexagons}

Following the generation of hexagonal centroids, the \texttt{gen\_hex\_coord()} function constructs the coordinates of each hexagonal bin by defining its six polygonal vertices. These coordinates are used to visualize the hexagonal tessellation.

Each hexagon is defined relative to its centroid \((C_x, C_y)\), with six vertices positioned equidistantly around the center. The function first verifies the presence of the required hexagon width parameter \texttt{a1}. This width determines the horizontal spacing (\texttt{hs}).

Two derived constants are calculated to define the relative distances to the vertices. The horizontal and vertical offset is defined as \(dx = a_1/2\), and \(dy = a_1/\sqrt{3}\) repectively. A vertical spacing factor \(vf = a_1/2\sqrt{3}\) refines vertical placement in staggered rows.

With these values, the function determines fixed offsets in the x and y directions for all six vertices relative to the centroid. These offsets form two vectors corresponding to the six compass directions used to define the polygon shape: top, top-left, bottom-left, bottom, bottom-right, and top-right.

For each centroid, six vertices are computed and assigned a polygon ID as the centroid. These vertices are then combined into a tibble that records the polygon ID (\texttt{h}) and the respective x (\texttt{x}) and y (\texttt{y}) coordinates for all hexagon corners.

To reduce computational overhead, the geometry calculations are implemented in C++ using \texttt{gen\_hex\_coord\_cpp()}, which returns a \texttt{tibble} of vertex coordinates.

\begin{verbatim}
all_hex_coord <- gen_hex_coord(
  centroids_data = all_centroids_df, 
  a1 = bin_configs$a1
  )

head(all_hex_coord, 5)
\end{verbatim}

\begin{verbatim}
>   h           x           y
> 1 1 -0.10000000 -0.09858838
> 2 1 -0.12471766 -0.11285913
> 3 1 -0.12471766 -0.14140063
> 4 1 -0.10000000 -0.15567138
> 5 1 -0.07528234 -0.14140063
\end{verbatim}

\paragraph{Assigning data points to their respective hexagons}\label{assigning-data-points-to-their-respective-hexagons}

After generating the centroids that define the hexagonal grid, the next step is to assign each point in the NLDR embedding to its nearest hexagonal bin. The \texttt{assign\_data()} function performs this assignment by calculating the $2\text{-}D$ Euclidean distance between each point in the $2\text{-}D$ embedding and all hexagon centroids.

First, the function extracts the first two dimensions of the scaled NLDR embedding, which represent the $2\text{-}D$ layout. It then selects the corresponding x and y coordinates of each hexagon's centroid.

Both the embedding coordinates and the centroid coordinates are converted to matrices to facilitate distance computations. The function uses the \texttt{proxy::dist()} method to compute a pairwise Euclidean distance matrix between all NLDR points and all centroids. For each NLDR point, the function identifies the index of the centroid with the smallest distance representing the closest hexagon---and assigns the corresponding hexagon ID (\texttt{h}) to the point.

The result is a \texttt{tibble} of the scaled $2\text{-}D$ embedding with an additional \texttt{h} column, indicating the hexagonal bin to which each point belongs.

\begin{verbatim}
umap_hex_id <- assign_data(
  nldr_scaled_obj = scurve_umap_obj, 
  centroids_data = all_centroids_df
  )

head(umap_hex_id, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 4
>    emb1  emb2    ID     h
>   <dbl> <dbl> <int> <int>
> 1 0.300 0.985     1   659
> 2 0.721 0.688     2   492
> 3 0.722 0.542     3   418
> 4 0.196 1.01      4   657
> 5 0.240 1.06      5   708
\end{verbatim}

\paragraph{Computing the standardized number of points within each hexagon}\label{computing-the-standardized-number-of-points-within-each-hexagon}

The \texttt{compute\_std\_counts()} function calculates both the raw and standardized counts of points inside each hexagon.

The function begins by grouping the data by hexagon ID (\texttt{h}) and counting the number of NLDR points falling within each bin. These raw counts are stored as \texttt{n\_h}. To enable comparisons across bins with varying densities, the function then standardizes these counts by dividing each bin's count by the maximum count across all bins. This yields a standardized bin counts, \texttt{w\_h}, ranging from \(0\) to \(1\).

\begin{verbatim}
std_df <- compute_std_counts(
  scaled_nldr_h = umap_hex_id
  )

head(std_df, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 3
>       h   n_h   w_h
>   <int> <int> <dbl>
> 1    82     2 0.002
> 2    83     1 0.001
> 3   106     1 0.001
> 4   107     5 0.005
> 5   108     7 0.007
\end{verbatim}

\paragraph{Mapping the points to their corresponding hexagonal bins}\label{mapping-the-points-to-their-corresponding-hexagonal-bins}

The \texttt{group\_hex\_pts()} function extracts the list of data point identifiers (\texttt{ID}) assigned to each hexagon in the NLDR space.

The function first groups the input data by \texttt{h}, which represents the hexagon ID associated with each point in the $2\text{-}D$ layout. Within each group, it collects the \texttt{ID}s into a list, resulting in a summary where each row corresponds to a single hexagon. The resulting column, \texttt{pts\_list}, contains all point identifiers associated with that hexagon.

\begin{verbatim}
pts_df <- group_hex_pts(
  scaled_nldr_hexid = umap_hex_id
  )

head(pts_df, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 2
>       h pts_list 
>   <int> <list>   
> 1    82 <int [2]>
> 2    83 <int [1]>
> 3   106 <int [1]>
> 4   107 <int [5]>
> 5   108 <int [7]>
\end{verbatim}

\subsubsection{Obtaining bin centroids}\label{obtaining-bin-centroids}

The \texttt{merge\_hexbin\_centroids()} function combines hexagonal bin coordinates, raw and standardized counts within each hexagons.

This function begins by arranging the \texttt{counts\_data} by \texttt{h} to ensure consistent ordering. It then performs a full join with \texttt{centroids\_data}, aligning hexagon IDs (\texttt{h}) between the two datasets to incorporate both hexagonal bin centroids (\texttt{h}) and count metrics. After merging, the function handles missing values in the count columns: any \texttt{NA} values in \texttt{w\_h} or \texttt{n\_h} are replaced with zeros. This ensures that hexagons with no assigned data points are retained in the output, with zero values for count-related fields. The resulting data contains the full set of hexagon centroids along with associated bin counts (\texttt{n\_h}) and standardized counts (\texttt{w\_h}).

\begin{verbatim}
df_bin_centroids <- merge_hexbin_centroids(
  centroids_data = all_centroids_df, 
  counts_data = hb_obj$std_cts
  )

head(df_bin_centroids, 5)
\end{verbatim}

\begin{verbatim}
>   h          c_x        c_y n_h w_h
> 1 1 -0.100000000 -0.1271299   0   0
> 2 2 -0.050564674 -0.1271299   0   0
> 3 3 -0.001129349 -0.1271299   0   0
> 4 4  0.048305977 -0.1271299   0   0
> 5 5  0.097741302 -0.1271299   0   0
\end{verbatim}

\subsubsection{Indicating neighbors by line segments connecting centroids}\label{indicating-neighbors-by-line-segments-connecting-centroids}

To represent the neighborhood structure of hexagonal bins in a $2\text{-}D$ layout, we employ Delaunay triangulation \citep{lee1980, albrecht2024} on the centroids of hexagons. This geometric approach is used to infer which bins are considered neighbors.

The \texttt{tri\_bin\_centroids()} function generates a triangulation object from the x and y coordinates of hexagon centroids using the \texttt{interp::tri.mesh()} function \citep{albrecht2024}. This triangulation forms the structural basis for identifying adjacent bins.

\begin{verbatim}
tr_object <- tri_bin_centroids(
  centroids_data = df_bin_centroids
  )
\end{verbatim}

The \texttt{gen\_edges()} function uses this triangulation object to extract line segments between neighboring bins. It constructs a unique set of bin-to-bin connections by identifying the triangle edges and filtering duplicate or reversed links. Each edge is then annotated with its start and end coordinates, and a Euclidean distance is computed using the helper function \texttt{calc\_2d\_dist()}.

\begin{verbatim}
trimesh <- gen_edges(tri_object = tr_object, a1 = hb_obj$a1)

head(trimesh, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 8
>    from    to   x_from  y_from     x_to    y_to from_count to_count
>   <int> <int>    <dbl>   <dbl>    <dbl>   <dbl>      <dbl>    <dbl>
> 1     1     2 -0.1     -0.127  -0.0506  -0.127           0        0
> 2    26    27 -0.0753  -0.0843 -0.0258  -0.0843          0        0
> 3     3    27 -0.00113 -0.127  -0.0258  -0.0843          0        0
> 4    26    52 -0.0753  -0.0843 -0.0506  -0.0415          0        0
> 5    52    53 -0.0506  -0.0415 -0.00113 -0.0415          0        0
\end{verbatim}

The \texttt{update\_trimesh\_index()} function re-indexes the node IDs to ensure that edge identifiers are sequentially numbered and consistent with downstream analysis.

\begin{verbatim}
trimesh <- update_trimesh_index(trimesh_data = trimesh)

head(trimesh, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 10
>    from    to   x_from  y_from     x_to    y_to from_count to_count
>   <int> <int>    <dbl>   <dbl>    <dbl>   <dbl>      <dbl>    <dbl>
> 1     1     2 -0.1     -0.127  -0.0506  -0.127           0        0
> 2    26    27 -0.0753  -0.0843 -0.0258  -0.0843          0        0
> 3     3    27 -0.00113 -0.127  -0.0258  -0.0843          0        0
> 4    26    52 -0.0753  -0.0843 -0.0506  -0.0415          0        0
> 5    52    53 -0.0506  -0.0415 -0.00113 -0.0415          0        0
> # i 2 more variables: from_reindexed <int>, to_reindexed <int>
\end{verbatim}

\subsubsection{Identifying and removing low-density hexagons}\label{identifying-and-removing-low-density-hexagons}

Not all hexagons contain meaningful information. Some may have very few or no data points due to the sparsity or shape of the underlying structure. Simply removing hexagons with low counts (e.g., fewer than a fixed threshold) can lead to gaps or ``holes'' in the $2\text{-}D$ structure, potentially disrupting the continuity of the representation.

To address this, we propose a more nuanced method that evaluates each hexagon not only based on its own density, but also in the context of its immediate neighbors. The \texttt{find\_low\_dens\_hex()} function identifies hexagonal bins with insufficient local support by calculating the average standardized count across their six neighboring bins. If this mean neighborhood density is below a user-defined threshold (e.g., \(0.05\)), the hexagon is flagged for removal.

The \texttt{find\_low\_dens\_hex()} function relies on a helper, \texttt{compute\_mean\_density\_hex()}, which iterates over all hexagons and computes the average density across neighbors based on their hexagon ID (\texttt{h}) and a defined number of bins along the x-axis (\texttt{b1}). The hexagonal layout assumes a fixed grid structure, so neighbor IDs are computed by positional offsets.

\begin{verbatim}
low_density_hex <- find_low_dens_hex(
  model_2d = df_bin_centroids, 
  b1 = 25, 
  md_thresh = 0.05
)
\end{verbatim}

For simplicity, we remove low-density hexagons using a threshold of \(0\).

\begin{verbatim}
df_bin_centroids <- df_bin_centroids |>
  dplyr::filter(n_h > 0)

trimesh <- trimesh |>
  dplyr::filter(from_count > 0,
                to_count > 0)

trimesh <- update_trimesh_index(trimesh)
\end{verbatim}

\subsection{Lifting the model into high dimensions}\label{lifting-the-model-into-high-dimensions}

The final step involves lifting the fitted $2\text{-}D$ model into $p\text{-}D$. This is done by modelling a point in $p\text{-}D$ as the $p\text{-}D$ mean of data points in the $2\text{-}D$ centroid. This is performed using the \texttt{avg\_highd\_data()} function, which takes $p\text{-}D$ data (\texttt{highd\_data}) and embedding data with their corresponding hexagonal bin IDs as inputs (\texttt{scaled\_nldr\_hexid}).

\begin{verbatim}
df_bin <- avg_highd_data(
  highd_data = scurve, 
  scaled_nldr_hexid = hb_obj$data_hb_id
)

head(df_bin, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 8
>       h    x1    x2    x3       x4        x5       x6        x7
>   <int> <dbl> <dbl> <dbl>    <dbl>     <dbl>    <dbl>     <dbl>
> 1    82 0.989  1.78  1.13  0.00470  0.0142   -0.0547  -0.00428 
> 2    83 0.960  1.93  1.28  0.00278 -0.0180   -0.0678  -0.00260 
> 3   106 0.997  1.37  1.07  0.00154  0.00687   0.0289   0.00760 
> 4   107 0.975  1.46  1.20 -0.00497 -0.000595  0.0139   0.000537
> 5   108 0.931  1.64  1.36 -0.00706  0.00487  -0.00705  0.000383
\end{verbatim}

\subsection{Prediction}\label{prediction}

The \texttt{predict\_emb()} function is used to predict a point in a $2\text{-}D$ embedding for a new $p\text{-}D$ data point using the fitted model. This function is useful to predict $2\text{-}D$ embedding irrespective of the NLDR technique.

In the prediction process, first, the nearest $p\text{-}D$ model point is identified for the new $p\text{-}D$ data point by computing $p\text{-}D$ Euclidean distance. Then, the corresponding $2\text{-}D$ bin centroid mapping for the identified $p\text{-}D$ model point is determined. Finally, the coordinates of the identified $2\text{-}D$ bin centroid is used as the predicted NLDR embedding for the new $p\text{-}D$ data point.

To accelerate this process, the nearest-neighbor search is implemented in C++ using \texttt{Rcpp} via the internal function \texttt{compute\_highd\_dist()}.

\begin{verbatim}
predict_data <- predict_emb(
  highd_data = scurve, 
  model_2d = df_bin_centroids, 
  model_highd = df_bin
  )

head(predict_data, 5)
\end{verbatim}

\begin{verbatim}
> # A tibble: 5 x 4
>   pred_emb_1 pred_emb_2    ID pred_h
>        <dbl>      <dbl> <int>  <int>
> 1      0.295      0.986     1    659
> 2      0.716      0.686     2    492
> 3      0.740      0.558     3    418
> 4      0.197      0.986     4    657
> 5      0.246      1.07      5    708
\end{verbatim}

It is worth noting that while \texttt{predict\_emb()} provides a general approach that works across methods, some NLDR techniques have their own built-in prediction mechanisms. For example, UMAP \citep{tomasz2023} supports direct prediction of embeddings for new data once a model is fitted.

\subsection{Compute residuals and Root Within Bin Sum of Square (RWBSS)}\label{compute-residuals-and-root-within-bin-sum-of-square-rwbss}

Root Within Bin Sum of Square (RWBSS) are used as goodness of fit metrics for the model. These metrics can be computed using the \texttt{glance()} function, which provides a tidy output for evaluation.

The function requires both the fitted model object returned by \texttt{fit\_highd\_model()} and $p\text{-}D$ data to begin. The $p\text{-}D$ model output (\texttt{model\_highd}) is first renamed to avoid naming conflicts during subsequent data joins. It then uses the \texttt{predict\_emb()} function to assign each point in the $p\text{-}D$ dataset to a corresponding hexagon bin in the $2\text{-}D$ model, producing a prediction data frame that contains both the predicted bin assignment (\texttt{pred\_h}) and the original observation \texttt{ID}.

The function joins this prediction output with both the $p\text{-}D$ model and the $p\text{-}D$ data (to retrieve true coordinates). It then calculates squared differences between the original and predicted $p\text{-}D$ coordinates for each dimension, storing these as \texttt{error\_square\_x1}, \texttt{error\_square\_x2}, \ldots, up to the dimensionality of the data.

From these per-dimension errors, the function computes absolute error which is the sum of absolute differences across all dimensions and observations and the RWBSS which is the average of the total squared error per point.

These metrics are returned in a tibble as \texttt{Error} (absolute error) and \texttt{RWBSS} (root mean squared error). The computation of total absolute error and RWBSS is performed in C++ for efficiency using the internal \texttt{compute\_errors()} function.

\begin{verbatim}
glance(
  x = scurve_model_obj,
  highd_data = scurve
  )
\end{verbatim}

\begin{verbatim}
> # A tibble: 1 x 2
>   Error  RMSE
>   <dbl> <dbl>
> 1  196. 0.116
\end{verbatim}

Furthermore, \texttt{augment()} requires both the fitted model object returned by \texttt{fit\_highd\_model()} and $p\text{-}D$ data to begin. It extends the fitted model by adding prediction results and error diagnostics to the original $p\text{-}D$ data.

The function starts with the same process as is used in the \texttt{glance()} function to produce a predicted point in $p\text{-}D$ for each point in the $p\text{-}D$ dataset.

Next, the function computes residuals between each original coordinate (\texttt{x1}, \texttt{x2}, \ldots, \texttt{xp}) and the corresponding modeled coordinate (\texttt{model\_high\_d\_x1}, \ldots, \texttt{model\_high\_d\_xp}) across all dimensions. It calculates both squared errors and absolute errors per dimension. These are used to compute two aggregate diagnostic measures per point. First, the \texttt{row\_wise\_total\_error} which is the total squared error across all dimensions, and the \texttt{row\_wise\_abs\_error} which is the total absolute error across all dimensions.

The final output is a data frame that combines the original IDs, high-dimensional data, predicted bin IDs, modeled coordinates, residuals, row wise total error, absolute error for the fitted values, and row wise total absolute error for each observation. The augmented dataset is always returned as a \texttt{tibble::tibble} with the same number of rows as the passed dataset.

\begin{verbatim}
model_error <- augment(
  x = scurve_model_obj,
  highd_data = scurve
  )
\end{verbatim}

\subsection{Visualizations}\label{visualizations}

The package offers several $2\text{-}D$ visualizations, including:

\begin{itemize}
\tightlist
\item
  A full hexagonal grid,
\item
  A hexagonal grid that matches the data,
\item
  A full grid based on centroid triangulation,
\item
  A centroid triangulation grid that aligns with the data,
\item
  A triangular mesh for any provided set of points.
\end{itemize}

The generated $p\text{-}D$ model, overlaid with the data, can also be visualized using \texttt{show\_langevitour}. Additionally, it features a function for visualizing the $2\text{-}D$ projection of the fitted model overlaid on the data, called \texttt{plot\_proj}.

Furthermore, there are two interactive plots, \texttt{show\_link\_plots} and \texttt{show\_error\_link\_plots}, which are designed to help diagnose the model.

Each visualization can be generated using its respective function, as described in this section.

\subsubsection{Hexagonal grid}\label{hexagonal-grid}

The \texttt{geom\_hexgrid()} function introduces a custom \texttt{ggplot2} layer designed for visualizing hexagonal grid on a provided set of bin centroids.

To display the complete grid, users should supply all available bin centroids.

\begin{verbatim}
full_hexgrid <- ggplot() + 
  geom_hexgrid(
    data = hb_obj$centroids, 
    aes(x = c_x, y = c_y)
    ) 
\end{verbatim}

If the goal is to plot only the subset of hexagons that correspond to bins containing data points, then only the centroids associated with those bins should be passed.

\begin{verbatim}
data_hexgrid <- ggplot() + 
  geom_hexgrid(
    data = df_bin_centroids, 
    aes(x = c_x, y = c_y)
    ) 
\end{verbatim}

\subsubsection{Triangular mesh}\label{triangular-mesh}

The \texttt{geom\_trimesh()} function introduces a custom \texttt{ggplot2} layer designed for visualizing $2\text{-}D$ wireframe on a provided set of bin centroids.

To display the complete wireframe, users should supply all available bin centroids.

\begin{verbatim}
full_triangulation_grid <- ggplot() + 
  geom_trimesh(
    data = hb_obj$centroids, 
    aes(x = c_x, y = c_y)
    ) 
\end{verbatim}

If the goal is to plot only the subset of hexagons that correspond to bins containing data points, then only the centroids associated with those bins should be passed.

\begin{verbatim}
data_triangulation_grid <- ggplot() + 
  geom_trimesh(
    data = df_bin_centroids, 
    aes(x = c_x, y = c_y)
    ) 
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/geom-outputs-pdf-1} 

}

\caption{The outputs of `geom\_hexgrid` and `geom\_trimesh` include: (a) a complete hexagonal grid, (b) a hexagonal grid that corresponds with the data, (c) a full grid based on centroid triangulation, and (d) a centroid triangulation grid that aligns with the data.}\label{fig:geom-outputs-pdf}
\end{figure}

\subsubsection{\texorpdfstring{$p\text{-}D$ model visualization}{model visualization}}\label{model-visualization}

To visualize how well the $p\text{-}D$ model captures the underlying structure of the high-dimensional data, we provide a tour of the model in $p\text{-}D$ using the \texttt{show\_langevitour()} function. This function renders a dynamic projection of both the high-dimensional data and the model using the \texttt{langevitour} R package \citep{paul2023}.

Before plotting, the data needs to be organized into a combined format through the \texttt{comb\_data\_model()} function. This function takes three inputs: \texttt{highd\_data} (the high-dimensional observations), \texttt{model\_highd} (high-dimensional summaries for each bin), and \texttt{model\_2d} (the hexagonal bin centroids of the model). It returns a tidy data frame combining both the data and the model.

In this structure, the \texttt{type} variable distinguishes between original observations (\texttt{"data"}) and the bin-averaged model representation (\texttt{"model"}).

\begin{verbatim}
df_exe <- comb_data_model(
  highd_data = scurve, 
  model_highd = df_bin, 
  model_2d = df_bin_centroids
  )
\end{verbatim}

The \texttt{show\_langevitour()} function then renders the visualization using the \texttt{langevitour} interface, displaying both types of points in a dynamic tour. The \texttt{edge\_data} input defines connections between neighboring bins (i.e., the hexagonal edges) to visualize the model's structure.

\begin{verbatim}
show_langevitour(
  point_data = df_exe, 
  edge_data = trimesh
  )
\end{verbatim}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/unnamed-chunk-29-1} \caption{$2\text{-}D$ projections of the lifted high-dimensional wireframe model from the `Scurve` UMAP layout. Each panel (a1–a4) shows the model (black) overlaid on `Scurve` data (in purple) in different projections. These views illustrate how the lifted wireframe model captures the structure of the `Scurve` data. The two twists visible in the UMAP layout can also be seen in the lifted model.}\label{fig:unnamed-chunk-29}
\end{figure}

As an alternative to \texttt{langevitour}, users can explore the fitted $p\text{-}D$ model using the \texttt{detourr} \citep{casper2025}. The combined data object from \texttt{comb\_data\_model()} can be passed directly to the \texttt{detour()} function, where \texttt{tour\_aes()} defines the projection variables and color mapping. The visualization is rendered using \texttt{show\_scatter()}, which can display both data points and the model's structural edges via the \texttt{edges} argument.

\begin{verbatim}
library(detourr)

detour(
  df_exe,
  tour_aes(
    projection = starts_with("x"),
    colour = type
  )
) |>
  show_scatter(axes = TRUE, 
               edges = as.matrix(trimesh[, c("from_reindexed", "to_reindexed")]),
               palette = c("#66B2CC", "#FF7755"))
\end{verbatim}

In the resulting interactive visualization, blue points represent the high-dimensional data, orange points represent the model centroids from each bin, and the lines between model points reflect the $2\text{-}D$ wireframe structure mapped to high-dimensional space.

\subsubsection{Link plots}\label{link-plots}

There are mainly two interactive link plots can be generated.

To support interactive evaluation of how well the $p\text{-}D$ model captures the structure of the high-dimensional data, we introduce \texttt{show\_link\_plots()}. This visualization combines two complementary views: the nonlinear dimension reduction (NLDR) representation and a dynamic tour of the model ovelaid the data in the high-dimensional space. Both views are interactively linked, enabling users to explore.

Before visualization, the input data must be prepared using the \texttt{comb\_all\_data\_model()} function. This function combines the high-dimensional data (\texttt{highd\_data}), the NLDR data (\texttt{nldr\_data}), and the bin-averaged high-dimensional model representation (\texttt{model\_highd}) aligned to the $2\text{-}D$ bin layout (\texttt{model\_2d}):

This combined dataset includes both the original observations and the bin-level model averages, labeled with a \texttt{type} variable for distinguishing between them.

\begin{verbatim}
df_exe <- comb_all_data_model(
  highd_data = scurve, 
  nldr_data = scurve_umap, 
  model_highd = df_bin, 
  model_2d = df_bin_centroids
  )
\end{verbatim}

The function \texttt{show\_link\_plots()} generates two side-by-side, interactively linked plots; a $2\text{-}D$ NLDR representation, and a dynamic projection tour in the original high-dimensional space (using the \texttt{langevitour} package), displaying both the data and the model. The function takes the output from \texttt{comb\_all\_data\_model()} (\texttt{point\_data}) and \texttt{edge\_data} which defines connections between neighboring bins.

These two views are linked using \texttt{crosstalk}, allowing interactive selection of points in the NLDR plot to highlight corresponding structures in the \texttt{langevitour} output.

\begin{verbatim}
nldrdt_link <- show_link_plots(
  point_data = df_exe, 
  edge_data = trimesh, 
  point_colour = clr_choice
  )

class(nldrdt_link) <- c(class(nldrdt_link), "htmlwidget")

nldrdt_link
\end{verbatim}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/unnamed-chunk-33-1} \caption{Exploring the correspondence between UMAP layout and `Scurve` structure in $7\text{-}D$. Two sets of plots are linked: UMAP layout (a1, b1) and projection of $7\text{-}D$ model and data (a2, b2). The purple points indicate the selected subsets, which differ between rows. In (a1), the lower bridge of the `Scurve` is highlighted, which corresponds in (a2) to points spanning across both arms of the high-dimensional structure. In (b1), a different region near the upper arm of the `Scurve` is selected, and in (b2) these points map onto one side of the curved manifold in $7\text{-}D$ projection. While the UMAP layout suggests distinct local clusters, the linked tour views reveal how these selections trace continuous structures in the $7\text{-}D$ space, highlighting distortions introduced by UMAP.}\label{fig:unnamed-chunk-33}
\end{figure}

\texttt{show\_error\_link\_plots()} helps to see investigate whether the model fits the points everywhere or fits better in some places, or simply mismatches the pattern.

Before visualization, the input data must be prepared using the \texttt{comb\_all\_data\_model\_error()} function. The function requires several arguments: points data which contain high-dimensional data (\texttt{highd\_data}), NLDR data (\texttt{nldr\_data}), high-dimensional model data (\texttt{model\_highd}), $2\text{-}D$ model data (\texttt{model\_2d}), and model error (\texttt{error\_data}).

This combined dataset includes both the original observations and the bin-level model averages, labeled with a \texttt{type} variable for distinguishing between them.

\begin{verbatim}
df_exe <- comb_all_data_model_error(
  highd_data = scurve, 
  nldr_data = scurve_umap, 
  model_highd = df_bin, 
  model_2d = df_bin_centroids, 
  error_data = model_error
  )
\end{verbatim}

The function \texttt{show\_error\_link\_plots()} generates three side-by-side, interactively linked plots; a error distribution, a $2\text{-}D$ NLDR representation, and a dynamic projection tour in the original high-dimensional space (using the \texttt{langevitour} package), displaying both the data and the model. The function takes the output from \texttt{comb\_all\_data\_model\_error()} (\texttt{point\_data}) and \texttt{edge\_data} which defines connections between neighboring bins.

These two views are linked using \texttt{crosstalk}, allowing interactive selection of points in the NLDR plot to highlight corresponding structures in the high-dimensional projection. This setup facilitates the diagnosis of local distortion, structural artifacts, and model fit quality.

These three views are linked using \texttt{crosstalk}, allowing interactive selection of points in error plot and the NLDR plot to highlight corresponding structures in the \texttt{langevitour} output.

\begin{verbatim}
errornldrdt_link <- show_error_link_plots(
  point_data = df_exe, 
  edge_data = trimesh, 
  point_colour = clr_choice
)

class(errornldrdt_link) <- c(class(errornldrdt_link), "htmlwidget")

errornldrdt_link
\end{verbatim}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/unnamed-chunk-37-1} \caption{Exploring residuals in relation to UMAP layouts using a $7\text{-}D$ `Scurve` model. Three views are linked: distribution of residuals (a1, b1), UMAP layout (a2, b2), and projection of the $7\text{-}D$ model with data (a3, b3). The purple points highlight selected subsets of the data, which differ across rows. In the top row (a1–a3), points with higher residuals (a1) are selected, corresponding to the sparse bridging region in the UMAP layout (a2) and the less dense end of the `Scurve` in the high-dimensional projection (a3). In the bottom row (b1–b3), points with lower residuals (b1) are highlighted, which map to one side of the dense region in the NLDR layout (b2) and to a thicker band of the `Scurve` in the projection (b3). This comparison illustrates how residuals can help diagnose distortions in UMAP, with high-residual points often concentrated in sparse or stretched regions of the structure.}\label{fig:unnamed-chunk-37}
\end{figure}

As an alternative to generate tour, the link plots can be generate with \texttt{detourr} as well. To achieve this, users can construct the linked visualization manually using the \texttt{crosstalk} and \texttt{htmltools} packages. The two- and three-panel linked layouts are created by arranging the NLDR plot (\texttt{nldr\_plt}), the optional error distribution plot (\texttt{error\_plt}), and a dynamic high-dimensional projection view (\texttt{langevitour\_output} or \texttt{detourr\_output}) side by side within a flexible grid layout. The \texttt{bscols()} function from \texttt{crosstalk} synchronizes point selection between these plots, ensuring linked brushing across visual components.

\section{Application}\label{application}

Single-cell RNA sequencing (scRNA-seq) is a popular and powerful technology that allows you to profile the whole transcriptome of a large number of individual cells \citep{andrews2021}.

Clustering of single-cell data is used to identify groups of cells with similar expression profiles. NLDR often used to summarise the discovered clusters, and help to understand the results. The purpose of this example is to \emph{illustrate how to use our method to help decide on an appropriate NLDR layout that accurately represents the data}.

Limb muscle cells of mice in \citet{tabula2018} are examined. There are \(1067\) single cells, with \(14997\) gene expressions. Following their pre-processing, different NLDR methods were performed using ten principal components. Figure \ref{fig:limb-rwbss} (b) is the reproduction of the published plot. The question is whether this accurately represents the cluster structure in the data. Our method help to provide a better $2\text{-}D$ layout.

\begin{verbatim}
design <- gen_design(n_right = 6, ncol_right = 2)

plot_rmse_layouts(plots = list(error_plot_limb, nldr1, 
                               nldr2, nldr3, nldr4, 
                               nldr5, nldr6), design = design)
\end{verbatim}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/limb-rwbss-1} \caption{Assessing which of the 6 NLDR layouts on the limb muscle data is the better representation using RWBSS for varying binwidth ($a_1$). Colour  used for the lines and points in the left plot and in the scatterplots represents NLDR layout (a-f). Layout d is perform well at large binwidth (where the binwidth is not enough to capture the data struture) and poorly as bin width decreases. Layout f is the best choice.\label{fig:limb-rwbss}}\label{fig:limb-rwbss}
\end{figure}

\begin{figure}[H]
\includegraphics[width=1\linewidth]{paper-quollr_files/figure-latex/model-limb-1} \caption{Compare the published $2-\text{D}$ layout (Figure \ref{fig:limb-rwbss}b) and the $2-\text{D}$ layout selected (Figure \ref{fig:limb-rwbss}f) by RWBSS plot (Figure \ref{fig:limb-rwbss}) from the tSNE, UMAP, PHATE, TriMAP, and PaCMAP with different hyper-parameters. The Limb muscle data ($n =  1067$) has seven close different shaped clusters in $10\text{-}D$.}\label{fig:model-limb}
\end{figure}

\section{Discussion}\label{discussion}

The \texttt{quollr} package introduces a new framework for interpreting NLDR outputs by fitting a geometric wireframe model in $2\text{-}D$ and lifting it into high-dimensional space. This lifted model provides a direct way to assess how well a $2\text{-}D$ layout, produced by methods such as tSNE, UMAP, PHATE, TriMAP, or PaCMAP, preserves the structure of the original high-dimensional data. The approach offers both numerical and visual diagnostics to support the selection of NLDR methods and tuning hyper-parameters that produce the most accurate $2\text{-}D$ representations.

In contrast to the common practice of visually inspecting scatterplots for clusters or patterns, \texttt{quollr} provides a quantitative route for evaluation. It enables the computation of RWBSS and residuals between the original high-dimensional data and the lifted model, offering interpretable diagnostics. These diagnostics are complemented by interactive linked plots and high-dimensional dynamic visualizations using the \texttt{langevitour} package, allowing users to inspect where the model fits well and where it does not.

To support efficient computation, particularly for large-scale datasets, several core functions in \texttt{quollr} are implemented in C++ using \texttt{Rcpp} and \texttt{RcppArmadillo}. These include functions for computing Euclidean distances in high-dimensional and $2\text{-}D$ space, identifying nearest centroids, calculating residual errors, and generating polygonal coordinates of hexagons. For instance, \texttt{compute\_highd\_dist()} accelerates nearest neighbor lookup in high-dimensional space, \texttt{compute\_errors()} calculates RWBSS and total absolute error efficiently, and \texttt{calc\_2d\_dist\_cpp()} speeds up distance calculations in $2\text{-}D$. Additionally, \texttt{gen\_hex\_coord\_cpp()} constructs the coordinates for hexagonal bins based on their centroids with minimal overhead. These optimizations result in substantial performance gains compared to native R implementations, making the package responsive even when used in interactive contexts or on large datasets such as single-cell transcriptomic profiles.

The modular structure of the package is designed to support both flexibility and reproducibility. Users can access individual functions to control each step of the pipeline such as scaling, binning, and triangulation or use the main function \texttt{fit\_highd\_model()} for end-to-end model construction. The diagnostics can be used not only to compare NLDR methods but also to tune binning parameters, assess layout stability, and detect local distortions in the embedding.

There are several avenues for future development. While hexagonal binning provides a regular structure conducive to modeling, alternative spatial discretizations (e.g., adaptive binning or density-aware tessellations) could be explored to better capture varying data densities. Expanding support for additional distance metrics in the lifting and prediction steps may improve performance across different domains. Additionally, statistical inference tools could be introduced to assess the stability and robustness of the fitted model, which would enhance interpretability and confidence in the outcomes.

\section{Acknowledgements}\label{acknowledgements}

The source code for reproducing this paper can be found at: \url{https://github.com/JayaniLakshika/paper-quollr}.

This article is created using \CRANpkg{knitr} \citep{yihui2015} and \CRANpkg{rmarkdown} \citep{yihui2018} in R with the \texttt{rjtools::rjournal\_article} template. These \texttt{R} packages were used for this work: \texttt{cli} \citep{gabor2025}, \texttt{dplyr} \citep{hadley2023}, \texttt{ggplot2} \citep{hadley2016}, \texttt{interp} (\textgreater= 1.1-6) \citep{albrecht2024}, \texttt{langevitour} \citep{paul2023}, \texttt{proxy}\citep{david2022}, \texttt{stats} \citep{core2025}, \texttt{tibble} \citep{kirill2023}, \texttt{tidyselect} \citep{lionel2024}, \texttt{crosstalk} \citep{joe2023}, \texttt{plotly} \citep{chapman2020}, \texttt{kableExtra} \citep{hao2024}, \texttt{patchwork} \citep{thomas2024}, and \texttt{readr} \citep{hadley2024}.

\bibliography{paper-quollr.bib}

\address{%
Jayani P. Gamage\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
\url{https://jayanilakshika.netlify.app/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-6265-6481}{0000-0002-6265-6481}}\\%
\href{mailto:jayani.piyadigamage@monash.edu}{\nolinkurl{jayani.piyadigamage@monash.edu}}%
}

\address{%
Dianne Cook\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
\url{http://www.dicook.org/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3813-7155}{0000-0002-3813-7155}}\\%
\href{mailto:dicook@monash.edu}{\nolinkurl{dicook@monash.edu}}%
}

\address{%
Paul Harrison\\
Monash University\\%
MGBP, BDInstitute, VIC 3800 Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3980-268X}{0000-0002-3980-268X}}\\%
\href{mailto:paul.harrison@monash.edu}{\nolinkurl{paul.harrison@monash.edu}}%
}

\address{%
Michael Lydeamore\\
Monash University\\%
Department of Econometrics and Business Statistics, VIC 3800 Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0001-6515-827X}{0000-0001-6515-827X}}\\%
\href{mailto:michael.lydeamore@monash.edu}{\nolinkurl{michael.lydeamore@monash.edu}}%
}

\address{%
Thiyanga S. Talagala\\
University of Sri Jayewardenepura\\%
Department of Statistics, Gangodawila, Nugegoda 10100 Sri Lanka\\
%
\url{https://thiyanga.netlify.app/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0002-0656-9789}{0000-0002-0656-9789}}\\%
\href{mailto:ttalagala@sjp.ac.lk}{\nolinkurl{ttalagala@sjp.ac.lk}}%
}
